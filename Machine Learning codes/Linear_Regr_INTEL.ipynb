{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pBfpW63HWSI",
        "outputId": "42776c40-0b7c-4cc4-cc24-d174ddae3913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyts\n",
            "  Downloading pyts-0.13.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.3.2)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.10/dist-packages (from pyts) (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.2->pyts) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.2->pyts) (67.7.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->pyts) (3.2.0)\n",
            "Installing collected packages: pyts\n",
            "Successfully installed pyts-0.13.0\n",
            "Requirement already satisfied: pyts in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.3.2)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.10/dist-packages (from pyts) (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.2->pyts) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.2->pyts) (67.7.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->pyts) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyts\n",
        "!pip install --upgrade pyts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJlqeYmMEn7A",
        "outputId": "542ecfc5-4176-4dd5-8864-d44b8aa04a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.fft import fft, dct\n",
        "import pywt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "from pyts.approximation import PiecewiseAggregateApproximation as PAA\n",
        "from scipy.fftpack import idct\n",
        "\n",
        "def generate_random_vectors(n, r):\n",
        "    random_vectors = np.random.normal(0, 1, size=(r, n))\n",
        "    return random_vectors\n",
        "\n",
        "def MINE(data,R,R_inv):\n",
        "\n",
        "  prod = np.matmul(data,R)\n",
        "  bitmap = np.where(prod < 0, 0, 1)\n",
        "  recon = np.matmul(bitmap,R_inv)\n",
        "  return recon\n",
        "\n",
        "def plot_confidence_interval(scores,method,stride,compression,type):\n",
        "    # compute mean silhouette score and confidence interval\n",
        "    mean_score = np.mean(scores)\n",
        "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean_score, scale=stats.sem(scores))\n",
        "\n",
        "    # 2D array with the lower and upper bounds of the confidence interval\n",
        "    yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "    # create the figure and axis objects\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # plot the data with error bars\n",
        "    ax.plot(scores, marker='o')\n",
        "    ax.errorbar(x=range(len(scores)), y=scores, yerr=yerr, fmt='none', ecolor='r')\n",
        "\n",
        "    # set the axis labels and title\n",
        "    ax.set_xlabel('Subset of features')\n",
        "\n",
        "    if type == 'SC' :\n",
        "     ax.set_ylabel('R2 score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'R2-scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'R2-scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "\n",
        "\n",
        "    if type == 'RMSE' :\n",
        "     ax.set_ylabel('RMSE-score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'RMSE-scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'RMSE-scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "    # return the figure object\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_score_confidence_interval(data,window_size,compression):\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))  # Adjust the figure size as needed\n",
        "\n",
        "    colors = ['b', 'g', 'r', 'c', 'm']  # Colors for each method\n",
        "\n",
        "    y_axis_limits = (-1.5, 1.5)  # Outlier limits\n",
        "\n",
        "    for i, (scores, method) in enumerate(data):\n",
        "        ax = axs[i]\n",
        "        n = len(scores)\n",
        "        x = np.arange(n) + 1\n",
        "        y = np.array(scores)\n",
        "\n",
        "        # Identify and count outliers based on the limits\n",
        "        outliers = np.where((y < -1.5) | (y > 1.5))[0]\n",
        "        num_outliers = len(outliers)\n",
        "\n",
        "        # Exclude outliers from the data\n",
        "        filtered_scores = np.delete(y, outliers)\n",
        "        filtered_x = np.delete(x, outliers)\n",
        "\n",
        "        mean = np.mean(filtered_scores)\n",
        "        # Calculate the confidence interval\n",
        "        ci = stats.t.interval(0.95, len(filtered_scores), loc=mean, scale=stats.sem(filtered_scores))\n",
        "        # Plot the non-outlier data points and mean\n",
        "        ax.plot(filtered_x, filtered_scores, 'o', markersize=4, color=colors[i])\n",
        "        ax.plot(x, [mean] * n, '--', color=colors[i])\n",
        "\n",
        "        fig.suptitle(f'R2-scores with 95% confidence interval // W= {window_size}, C= {compression} ')\n",
        "\n",
        "        ax.set_xlabel('Window')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title(f'{method} : [{ci[0]:.5f}, {ci[1]:.5f}] ')\n",
        "        ax.grid(True)\n",
        "\n",
        "        # Set the y-axis limits to the predefined range\n",
        "        ax.set_ylim(y_axis_limits)\n",
        "\n",
        "        # Add the number of outliers as text in the top right corner\n",
        "        ax.text(0.85, 0.9, f'Outliers: {num_outliers}', transform=ax.transAxes, fontsize=10, color='red', ha='right')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_RMSE_confidence_interval(data, window_size, compression, outlier_threshold=3):\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))  # Adjust the figure size as needed\n",
        "\n",
        "    colors = ['b', 'g', 'r', 'c', 'm']  # Colors for each method\n",
        "\n",
        "    y_axis_limits = (-1.5, 1.5) # Outlier limits\n",
        "\n",
        "    for i, (scores, method) in enumerate(data):\n",
        "        ax = axs[i]\n",
        "        n = len(scores)\n",
        "        x = np.arange(n) + 1\n",
        "        y = np.array(scores)\n",
        "        # Identify and count outliers based on the limits\n",
        "        outliers = np.where((y < -1.5) | (y > 1.5))[0]\n",
        "        num_outliers = len(outliers)\n",
        "\n",
        "        # Exclude outliers from the data\n",
        "        filtered_scores = np.delete(y, outliers)\n",
        "        filtered_x = np.delete(x, outliers)\n",
        "\n",
        "        mean = np.mean(filtered_scores)\n",
        "        # Calculate the confidence interval\n",
        "        ci = stats.t.interval(0.95, len(filtered_scores), loc=mean, scale=stats.sem(filtered_scores))\n",
        "\n",
        "        # Plot the non-outlier data points and mean\n",
        "        ax.plot(filtered_x, filtered_scores, 'o', markersize=4, color=colors[i])\n",
        "        ax.plot(x, [mean] * n, '--', color=colors[i])\n",
        "\n",
        "        fig.suptitle(f'RMSE-scores with 95% confidence interval // W= {window_size}, C= {compression} ')\n",
        "\n",
        "        ax.set_xlabel('Window')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title(f'{method} : [{0}, {ci[1]:.5f}]')\n",
        "        ax.grid(True)\n",
        "\n",
        "        # Set the y-axis limits to the predefined range\n",
        "        ax.set_ylim(y_axis_limits)\n",
        "\n",
        "        # Add the number of outliers as text in the top right corner\n",
        "        ax.text(0.85, 0.9, f'Outliers: {num_outliers}', transform=ax.transAxes, fontsize=10, color='red', ha='right')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "# Load data from file\n",
        "all_temperatures = np.loadtxt(\"/content/drive/MyDrive/temp.txt\", delimiter='\\t')\n",
        "all_humidities = np.loadtxt(\"/content/drive/MyDrive/humidity.txt\", delimiter='\\t')\n",
        "all_lights = np.loadtxt(\"/content/drive/MyDrive/light.txt\", delimiter='\\t')\n",
        "all_voltages = np.loadtxt(\"/content/drive/MyDrive/voltage.txt\", delimiter='\\t')\n",
        "\n",
        "alldata = [all_temperatures.T , all_humidities.T , all_lights.T , all_voltages.T ]\n",
        "\n",
        "\n",
        "count = 0\n",
        "for data in alldata:\n",
        "\n",
        "  # window size\n",
        "  window_sizes = [16, 32, 64, 128]\n",
        "  compressions = [4, 8, 16]\n",
        "  methods = ['DFT', 'DCT', 'DWT', 'PAA','MINE']\n",
        "  if count == 0:\n",
        "    dstring = 'Temperatures'\n",
        "  elif count == 1:\n",
        "     dstring = 'Humidities'\n",
        "  elif count == 2:\n",
        "     dstring = 'Lights'\n",
        "  elif count == 3:\n",
        "    dstring = 'Voltages'\n",
        "  count = count + 1\n",
        "  #file for storing\n",
        "  folder_name = f\"Linear Regression {dstring}  Measurements\"\n",
        "\n",
        "  if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "\n",
        "  raw_folder_name = f\"RAW DATA\"\n",
        "  if not os.path.exists(os.path.join(folder_name,raw_folder_name)):\n",
        "    os.makedirs(os.path.join(folder_name,raw_folder_name))\n",
        "  pdf_file = os.path.join(folder_name,raw_folder_name, 'plots.pdf')\n",
        "\n",
        "  with PdfPages(pdf_file, 'a') as pdf:\n",
        "    for window_size in window_sizes:\n",
        "\n",
        "      # lists to store metrics for each window\n",
        "      rmse_list = []\n",
        "      #mae_list = []\n",
        "      r2_list = []\n",
        "      num =int( window_size+window_size/4)\n",
        "\n",
        "      for i in range(0, data.shape[1]-window_size*2+1, window_size):\n",
        "            # Split data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(data[:, i:i+window_size], data[:, i+window_size:i+num], test_size=0.25, random_state=13)\n",
        "\n",
        "            #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "            # Data scaling\n",
        "            sc = MinMaxScaler()\n",
        "            X_train_scaled = sc.fit_transform(X_train)\n",
        "            X_test_scaled = sc.transform(X_test)\n",
        "            y_train_scaled = sc.fit_transform(y_train)\n",
        "            y_test_scaled = sc.transform(y_test)\n",
        "\n",
        "            # Model application\n",
        "            regressor = LinearRegression()\n",
        "            regressor.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "            # Generate predictions\n",
        "            y_pred_scaled = regressor.predict(X_test_scaled)\n",
        "\n",
        "            # performance (mean squared error, mean absolute error, and R-squared score)\n",
        "            mse = mean_squared_error(y_test_scaled, y_pred_scaled)\n",
        "            rmse = np.sqrt(mse)\n",
        "            #mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
        "            r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
        "            rmse_list.append(rmse)\n",
        "            r2_list.append(r2)\n",
        "\n",
        "      #store plots\n",
        "      methd = 'RAW'\n",
        "      comprsn = 0\n",
        "      fig = plot_confidence_interval(r2_list,methd,window_size ,comprsn,'SC') #plot function call\n",
        "      pdf.savefig(fig)\n",
        "      plt.close(fig)\n",
        "\n",
        "      fig2 = plot_confidence_interval(rmse_list,methd,window_size ,comprsn,'RMSE') #plot function call\n",
        "      pdf.savefig(fig2)\n",
        "      plt.close(fig2)\n",
        "\n",
        "      # create a file name based on the method, subset index, stride, and compression\n",
        "      avg_r2 = np.mean(r2_list)\n",
        "      file_name = f\"Window size({window_size}) - R2-scores.txt\"\n",
        "      with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "                f.write(f\"Average R2 Score: {avg_r2}\\n\")\n",
        "                content = str(r2_list)\n",
        "                f.write(content)\n",
        "                f.close()\n",
        "\n",
        "      avg_rmse = np.mean(rmse_list)\n",
        "      file_name = f\"Window size({window_size}) - RMSE scores).txt\"\n",
        "      with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "                f.write(f\"Average RMSE Score: {avg_rmse}\\n\")\n",
        "                content = str(rmse_list)\n",
        "                f.write(content)\n",
        "                f.close()\n",
        "\n",
        "\n",
        "  for window_size in window_sizes:\n",
        "\n",
        "      #file for storing\n",
        "      stride_folder_name = f\"stride({window_size})\"\n",
        "      if not os.path.exists(os.path.join(folder_name,stride_folder_name)):\n",
        "          os.makedirs(os.path.join(folder_name,stride_folder_name))\n",
        "\n",
        "      for compression in compressions:\n",
        "\n",
        "          # #vector and inverse vector arrays for our algorithm\n",
        "          n = 64*int(window_size/compression)  # Dimension of each random vector\n",
        "          r = window_size  # Number of random vectors to generate/rows\n",
        "          R = generate_random_vectors(n, r)\n",
        "          R_inv= np.linalg.pinv(R)\n",
        "        # print(R.shape,R_inv.shape)\n",
        "\n",
        "          #files for storing measurements\n",
        "          compression_folder_name = f\"compression({compression})\"\n",
        "          if not os.path.exists(os.path.join(folder_name,stride_folder_name, compression_folder_name)):\n",
        "              os.makedirs(os.path.join(folder_name,stride_folder_name, compression_folder_name))\n",
        "\n",
        "          #files for storing plots\n",
        "          plots_dir = os.path.join(folder_name,stride_folder_name, compression_folder_name, 'plots')\n",
        "          os.makedirs(plots_dir, exist_ok=True)\n",
        "          pdf_file = os.path.join(plots_dir, 'plots.pdf')\n",
        "\n",
        "          with PdfPages(pdf_file, 'a') as pdf:\n",
        "              results1 = []\n",
        "              results2 = []\n",
        "              for method in methods:\n",
        "\n",
        "                  slices = window_size // compression\n",
        "\n",
        "                  # lists to store metrics for each window\n",
        "                  rmse_list = []\n",
        "                  #mae_list = []\n",
        "                  r2_list = []\n",
        "                  num =int( window_size+window_size/4)\n",
        "\n",
        "                  # Iterate over the windows\n",
        "                  for i in range(0, data.shape[1]-window_size*2+1, window_size):\n",
        "                      #vector and inverse vector arrays for our algorithm\n",
        "                      # n = 64*int(window_size/compression)  # Dimension of each random vector\n",
        "                      # r = window_size  # Number of random vectors to generate/rows\n",
        "                      # R = generate_random_vectors(n, r)\n",
        "                      # R_inv= np.linalg.pinv(R)\n",
        "\n",
        "                      # Split data\n",
        "                      X_train, X_test, y_train, y_test = train_test_split(data[:, i:i+window_size], data[:, i+window_size:i+num], test_size=0.25, random_state=13)\n",
        "                      #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "                      # Data scaling\n",
        "                      sc = MinMaxScaler()\n",
        "                      X_train_scaled = sc.fit_transform(X_train)\n",
        "                      X_test_scaled = sc.transform(X_test)\n",
        "                      y_train_scaled = sc.fit_transform(y_train)\n",
        "                      y_test_scaled = sc.transform(y_test)\n",
        "\n",
        "                      if method == 'DFT':\n",
        "\n",
        "                        #array to store the restored data\n",
        "                        compressed_X_train_scaled = np.zeros_like(X_train_scaled)\n",
        "\n",
        "                        for i in range(36):\n",
        "                            # Compute abs DFT of each row\n",
        "                            dft_X_train_scaled  =  np.fft.fft(X_train_scaled[i])\n",
        "                            # Sort the DFT coefficients along each row by their magnitude\n",
        "                            sorted_indices = np.argsort(-np.abs(dft_X_train_scaled))[:slices]\n",
        "                            sorted_dft_X_train_scaled = dft_X_train_scaled[sorted_indices]\n",
        "                            # Keep  top  coeff\n",
        "                            top__dft_X_train_scaled = sorted_dft_X_train_scaled[:slices]\n",
        "                            #reconstruct the compressed dataset\n",
        "                            compressed_X_train_scaled[i] = np.fft.ifft(top__dft_X_train_scaled,window_size).real\n",
        "\n",
        "                            # print('be4 dft',X_train_scaled[i].shape,X_train_scaled[i])\n",
        "                            # print('after dft', dft_X_train_scaled .shape,dft_X_train_scaled )\n",
        "                            # print('top dft ',top__dft_X_train_scaled.shape,top__dft_X_train_scaled)\n",
        "                            # print('after',compressed_X_train_scaled.shape,compressed_X_train_scaled)\n",
        "                            # dfhdfgh\n",
        "\n",
        "                        #array to store the restored data\n",
        "                        compressed_X_test_scaled = np.zeros_like(X_test_scaled)\n",
        "\n",
        "                        for i in range(12):\n",
        "                            # Compute abs DFT of each row\n",
        "                            dft_X_test_scaled =  np.fft.fft(X_test_scaled[i])\n",
        "                            # Sort the DFT coefficients along each row by their magnitude\n",
        "                            sorted_indices = np.argsort(-np.abs(dft_X_test_scaled))[:slices]\n",
        "                            sorted_dft_X_test_scaled = dft_X_test_scaled[sorted_indices]\n",
        "                            # Keep  top  coeff\n",
        "                            top__dft_X_test_scaled = sorted_dft_X_test_scaled[:slices]\n",
        "                            #reconstruct the compressed dataset\n",
        "                            compressed_X_test_scaled[i] = np.fft.ifft(top__dft_X_test_scaled,window_size).real\n",
        "\n",
        "                      elif method == 'DCT':\n",
        "\n",
        "                        #array to store the restored data\n",
        "                        compressed_X_train_scaled = np.zeros_like(X_train_scaled)\n",
        "\n",
        "                        for i in range(36):\n",
        "                            # Compute abs DCT of each row\n",
        "                            dct_X_train_scaled  =  dct(X_train_scaled[i])\n",
        "                            # Sort the DCT coefficients along each row by their magnitude\n",
        "                            sorted_indices = np.argsort(-np.abs(dct_X_train_scaled))[:slices]\n",
        "                            sorted_dct_X_train_scaled = dct_X_train_scaled[sorted_indices]\n",
        "                            # Keep  top  coeff\n",
        "                            top__dct_X_train_scaled = sorted_dct_X_train_scaled[:slices]\n",
        "                            #reconstruct the compressed dataset\n",
        "                            compressed_X_train_scaled[i] = idct(top__dct_X_train_scaled ,type = 2, n = window_size).real\n",
        "\n",
        "                        #array to store the restored data\n",
        "                        compressed_X_test_scaled = np.zeros_like(X_test_scaled)\n",
        "\n",
        "                        for i in range(12):\n",
        "                            # Compute abs DCT of each row\n",
        "                            dct_X_test_scaled =  dct(X_test_scaled[i])\n",
        "                            # Sort the DCT coefficients along each row by their magnitude\n",
        "                            sorted_indices = np.argsort(-np.abs(dct_X_test_scaled))[:slices]\n",
        "                            sorted_dct_X_test_scaled = dct_X_test_scaled[sorted_indices]\n",
        "                            # Keep  top  coeff\n",
        "                            top__dct_X_test_scaled = sorted_dct_X_test_scaled[:slices]\n",
        "                            #reconstruct the compressed dataset\n",
        "                            compressed_X_test_scaled[i] = idct(top__dct_X_test_scaled ,type = 2 , n = window_size).real\n",
        "\n",
        "\n",
        "                      elif method == 'DWT':\n",
        "\n",
        "                        #array to store the restored data\n",
        "                        compressed_X_train_scaled = np.zeros_like(X_train_scaled)\n",
        "\n",
        "                        for i in range(36):\n",
        "                            # Apply DWT to each row using 'db1' wavelet\n",
        "                            cA, cD = pywt.dwt(X_train_scaled[i], 'db1')\n",
        "                            # Sort\n",
        "                            sorted_cD_subset_data = np.zeros_like(cD) #initialize storage array\n",
        "                            sorted_indices = np.argsort(-np.abs(cD))[:slices]\n",
        "                            sorted_cD_subset_data[sorted_indices] = cD[sorted_indices]\n",
        "                            # Perform inverse DWT to restore the row\n",
        "                            compressed_X_train_scaled[i] = pywt.idwt(cA, sorted_cD_subset_data, 'db1')\n",
        "\n",
        "                        #array to store the restored data\n",
        "                        compressed_X_test_scaled = np.zeros_like(X_test_scaled)\n",
        "\n",
        "                        for i in range(12):\n",
        "                            # Apply DWT to each row using 'db1' wavelet\n",
        "                            cA, cD = pywt.dwt(X_test_scaled[i], 'db1')\n",
        "                            # Sort\n",
        "                            sorted_cD_subset_data = np.zeros_like(cD) #initialize storage array\n",
        "                            sorted_indices = np.argsort(-np.abs(cD))[:slices]\n",
        "                            sorted_cD_subset_data[sorted_indices] = cD[sorted_indices]\n",
        "                            # Perform inverse DWT to restore the row\n",
        "                            compressed_X_test_scaled[i] = pywt.idwt(cA, sorted_cD_subset_data, 'db1')\n",
        "\n",
        "                      elif method == 'PAA':\n",
        "                        # Apply PAA along the rows of the array\n",
        "                        paa = PAA(window_size = compression)\n",
        "                        compressed_X_train_scaled = paa.fit_transform(X_train_scaled)\n",
        "                        compressed_X_test_scaled = paa.fit_transform(X_test_scaled)\n",
        "\n",
        "                      elif method == 'MINE':\n",
        "\n",
        "                        #array to store the restored data\n",
        "                        compressed_X_train_scaled = np.zeros_like(X_train_scaled)\n",
        "\n",
        "                        for i in range(36):\n",
        "                          row = X_train_scaled[i]\n",
        "                          # Apply my algorithm along the rows of the array\n",
        "                          compressed_X_train_scaled[i] = MINE(row,R,R_inv)\n",
        "\n",
        "                        #array to store the restored data\n",
        "                        compressed_X_test_scaled = np.zeros_like(X_test_scaled)\n",
        "                        for i in range(12):\n",
        "                          row = X_test_scaled[i]\n",
        "                          # Apply my algorithm along the rows of the array\n",
        "                          compressed_X_test_scaled[i] = MINE(row,R,R_inv)\n",
        "\n",
        "                      # Model application\n",
        "                      regressor = LinearRegression()\n",
        "                      regressor.fit(compressed_X_train_scaled, y_train_scaled)\n",
        "\n",
        "                      # Generate predictions\n",
        "                      y_pred_scaled = regressor.predict(compressed_X_test_scaled)\n",
        "\n",
        "                      # Transform scaled predictions back to og scale\n",
        "                      #y_pred = sc.inverse_transform(y_pred_scaled)\n",
        "\n",
        "                      # performance (mean squared error, mean absolute error, and R-squared score)\n",
        "                      mse = mean_squared_error(y_test_scaled, y_pred_scaled)\n",
        "                      rmse = np.sqrt(mse)\n",
        "                      #mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
        "                      r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
        "\n",
        "                      # Add to list of measurements\n",
        "                      rmse_list.append(rmse)\n",
        "                      #mae_list.append(mae)\n",
        "                      r2_list.append(r2)\n",
        "\n",
        "                  results1.append((r2_list, method))\n",
        "                  results2.append((rmse_list, method))\n",
        "\n",
        "                  # create a file name based on the method, subset index, stride, and compression\n",
        "                  avg_r2 = np.mean(r2_list)\n",
        "                  file_name = f\"{method}_window size({window_size})_compression({compression} - R2 SCORES).txt\"\n",
        "                  # write the silhouette coefficients to file\n",
        "                  with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "                                f.write(f\"Average R2 Score: {avg_r2}\\n\")\n",
        "                                content = str(r2_list)\n",
        "                                f.write(content)\n",
        "                                f.close()\n",
        "\n",
        "                  avg_RMSE = np.mean(rmse_list)\n",
        "                  file_name = f\"{method}_window size({window_size})_compression({compression} - RMSEs).txt\"\n",
        "                  # write the silhouette coefficients to file\n",
        "                  with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "                                f.write(f\"Average RMSE Score: {avg_RMSE}\\n\")\n",
        "                                content = str(rmse_list)\n",
        "                                f.write(content)\n",
        "                                f.close()\n",
        "              #store plots\n",
        "              fig = plot_score_confidence_interval(results1,window_size,compression) #plot function call\n",
        "              pdf.savefig(fig)\n",
        "              plt.close(fig)\n",
        "              fig = plot_RMSE_confidence_interval(results2,window_size,compression) #plot function call\n",
        "              pdf.savefig(fig)\n",
        "              plt.close(fig)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "epUj6vNIHWDc"
      },
      "outputs": [],
      "source": [
        "#@title MAIN BACKUP\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.fft import fft, dct\n",
        "import pywt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "from pyts.approximation import PiecewiseAggregateApproximation as PAA\n",
        "from scipy.fftpack import idct\n",
        "\n",
        "\n",
        "\n",
        "def plot_confidence_interval(scores,method,stride,compression,type):\n",
        "    # compute mean silhouette score and confidence interval\n",
        "    mean_score = np.mean(scores)\n",
        "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean_score, scale=stats.sem(scores))\n",
        "\n",
        "    # 2D array with the lower and upper bounds of the confidence interval\n",
        "    yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "    # create the figure and axis objects\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # plot the data with error bars\n",
        "    ax.plot(scores, marker='o')\n",
        "    ax.errorbar(x=range(len(scores)), y=scores, yerr=yerr, fmt='none', ecolor='r')\n",
        "\n",
        "    # set the axis labels and title\n",
        "    ax.set_xlabel('Subset of features')\n",
        "\n",
        "    if type == 'SC' :\n",
        "     ax.set_ylabel('R2 score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'R2 scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'R2 scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "\n",
        "\n",
        "    if type == 'RMSE' :\n",
        "     ax.set_ylabel('RMSE score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'RMSE scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'RMSE scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "    # return the figure object\n",
        "    return fig\n",
        "\n",
        "\n",
        "# Load data from file\n",
        "all_temperatures = np.loadtxt(\"/content/drive/MyDrive/temp.txt\", delimiter='\\t')\n",
        "all_humidities = np.loadtxt(\"/content/drive/MyDrive/humidity.txt\", delimiter='\\t')\n",
        "all_lights = np.loadtxt(\"/content/drive/MyDrive/light.txt\", delimiter='\\t')\n",
        "all_voltages = np.loadtxt(\"/content/drive/MyDrive/voltage.txt\", delimiter='\\t')\n",
        "\n",
        "data = all_temperatures.T\n",
        "\n",
        "# window size\n",
        "window_sizes = [16, 32, 64, 128]\n",
        "compressions = [4, 8, 16]\n",
        "methods = ['DFT', 'DCT', 'DWT', 'PAA']\n",
        "\n",
        "#file for storing\n",
        "folder_name = f\"Linear Regression temperatures Measurements\"\n",
        "if not os.path.exists(folder_name):\n",
        "   os.makedirs(folder_name)\n",
        "\n",
        "raw_folder_name = f\"RAW DATA\"\n",
        "if not os.path.exists(os.path.join(folder_name,raw_folder_name)):\n",
        "   os.makedirs(os.path.join(folder_name,raw_folder_name))\n",
        "pdf_file = os.path.join(folder_name,raw_folder_name, 'plots.pdf')\n",
        "\n",
        "with PdfPages(pdf_file, 'a') as pdf:\n",
        "  for window_size in window_sizes:\n",
        "\n",
        "     # lists to store metrics for each window\n",
        "     rmse_list = []\n",
        "     #mae_list = []\n",
        "     r2_list = []\n",
        "     num =int( window_size+window_size/4)\n",
        "\n",
        "     for i in range(0, data.shape[1]-window_size*2+1, window_size):\n",
        "          # Split data\n",
        "          X_train, X_test, y_train, y_test = train_test_split(data[:, i:i+window_size], data[:, i+window_size:i+num], test_size=0.25, random_state=13)\n",
        "\n",
        "          #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "          # Data scaling\n",
        "          sc = MinMaxScaler()\n",
        "          X_train_scaled = sc.fit_transform(X_train)\n",
        "          X_test_scaled = sc.transform(X_test)\n",
        "          y_train_scaled = sc.fit_transform(y_train)\n",
        "          y_test_scaled = sc.transform(y_test)\n",
        "\n",
        "          # Model application\n",
        "          regressor = LinearRegression()\n",
        "          regressor.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "          # Generate predictions\n",
        "          y_pred_scaled = regressor.predict(X_test_scaled)\n",
        "\n",
        "          # performance (mean squared error, mean absolute error, and R-squared score)\n",
        "          mse = mean_squared_error(y_test_scaled, y_pred_scaled)\n",
        "          rmse = np.sqrt(mse)\n",
        "          #mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
        "          r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
        "          rmse_list.append(rmse)\n",
        "          r2_list.append(r2)\n",
        "\n",
        "     #store plots\n",
        "     methd = 'RAW'\n",
        "     comprsn = 0\n",
        "     fig = plot_confidence_interval(r2_list,methd,window_size ,comprsn,'SC') #plot function call\n",
        "     pdf.savefig(fig)\n",
        "     plt.close(fig)\n",
        "\n",
        "     fig2 = plot_confidence_interval(rmse_list,methd,window_size ,comprsn,'RMSE') #plot function call\n",
        "     pdf.savefig(fig2)\n",
        "     plt.close(fig2)\n",
        "\n",
        "     # create a file name based on the method, subset index, stride, and compression\n",
        "     avg_r2 = np.mean(r2_list)\n",
        "     file_name = f\"Window size({window_size}) - R2-scores.txt\"\n",
        "     with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Average R2 Score: {avg_r2}\\n\")\n",
        "              content = str(r2_list)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "     avg_rmse = np.mean(rmse_list)\n",
        "     file_name = f\"Window size({window_size}) - RMSE scores).txt\"\n",
        "     with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Average RMSE Score: {avg_rmse}\\n\")\n",
        "              content = str(rmse_list)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "\n",
        "for window_size in window_sizes:\n",
        "\n",
        "    #file for storing\n",
        "    stride_folder_name = f\"stride({window_size})\"\n",
        "    if not os.path.exists(os.path.join(folder_name,stride_folder_name)):\n",
        "        os.makedirs(os.path.join(folder_name,stride_folder_name))\n",
        "\n",
        "    for compression in compressions:\n",
        "\n",
        "        #files for storing measurements\n",
        "        compression_folder_name = f\"compression({compression})\"\n",
        "        if not os.path.exists(os.path.join(folder_name,stride_folder_name, compression_folder_name)):\n",
        "            os.makedirs(os.path.join(folder_name,stride_folder_name, compression_folder_name))\n",
        "\n",
        "        #files for storing plots\n",
        "        plots_dir = os.path.join(folder_name,stride_folder_name, compression_folder_name, 'plots')\n",
        "        os.makedirs(plots_dir, exist_ok=True)\n",
        "        pdf_file = os.path.join(plots_dir, 'plots.pdf')\n",
        "\n",
        "        with PdfPages(pdf_file, 'a') as pdf:\n",
        "            for method in methods:\n",
        "\n",
        "                slices = window_size // compression\n",
        "\n",
        "                # lists to store metrics for each window\n",
        "                rmse_list = []\n",
        "                #mae_list = []\n",
        "                r2_list = []\n",
        "                num =int( window_size+window_size/4)\n",
        "                print(data.shape[1])\n",
        "                # Iterate over the windows\n",
        "                for i in range(0, data.shape[1]-window_size*2+1, window_size):\n",
        "\n",
        "                    # Split data\n",
        "                    X_train, X_test, y_train, y_test = train_test_split(data[:, i:i+window_size], data[:, i+window_size:i+num], test_size=0.25, random_state=13)\n",
        "                    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "                    # Data scaling\n",
        "                    sc = MinMaxScaler()\n",
        "                    X_train_scaled = sc.fit_transform(X_train)\n",
        "                    X_test_scaled = sc.transform(X_test)\n",
        "                    y_train_scaled = sc.fit_transform(y_train)\n",
        "                    y_test_scaled = sc.transform(y_test)\n",
        "\n",
        "                    if method == 'DFT':\n",
        "\n",
        "                       # Compute abs DFT of each row of X_train\n",
        "                       dft_X_train_scaled = np.fft.fft(X_train_scaled, axis=1)\n",
        "                       abs_dft_X_train_scaled = np.abs(dft_X_train_scaled)\n",
        "                       # Sort ROWS\n",
        "                       sorted_dft_X_train_scaled = np.sort(abs_dft_X_train_scaled, axis=1)[:, ::-1]\n",
        "                       # Keep  top  coeff\n",
        "                       top4_dft_X_train_scaled = sorted_dft_X_train_scaled[:, :slices]\n",
        "                       #reconstruct the compressed dataset\n",
        "                       compressed_X_train_scaled = np.fft.ifft(top4_dft_X_train_scaled, axis=1).real\n",
        "\n",
        "                       # Compute abs DFT of each row of X_test\n",
        "                       dft_X_test_scaled = np.fft.fft(X_test_scaled, axis=1)\n",
        "                       abs_dft_X_test_scaled = np.abs(dft_X_test_scaled)\n",
        "                       # Sort ROWS\n",
        "                       sorted_dft_X_test_scaled = np.sort(abs_dft_X_test_scaled, axis=1)[:, ::-1]\n",
        "                       # Keep  top  coeff\n",
        "                       top4_dft_X_test_scaled = sorted_dft_X_test_scaled[:, :slices]\n",
        "                       #reconstruct the compressed dataset\n",
        "                       compressed_X_test_scaled = np.fft.ifft(top4_dft_X_test_scaled, axis=1).real\n",
        "\n",
        "                    elif method == 'DCT':\n",
        "\n",
        "                       # Compute DCT of each row of X_train\n",
        "                       dct_X_train_scaled = dct(X_train_scaled, axis=1)\n",
        "                       # Sort ROWS\n",
        "                       sorted_dct_X_train_scaled = np.sort(np.abs(dct_X_train_scaled), axis=1)[:, ::-1]\n",
        "                       # Keep  top  coeff\n",
        "                       top4_dct_X_train_scaled = sorted_dct_X_train_scaled[:, :slices]\n",
        "                       #reconstruct the compressed dataset\n",
        "                       compressed_X_train_scaled = idct(top4_dct_X_train_scaled, axis=1).real\n",
        "\n",
        "                       # Compute abs DFT of each row of X_test\n",
        "                       dct_X_test_scaled = dct(X_test_scaled, axis=1)\n",
        "                       # Sort ROWS\n",
        "                       sorted_dct_X_test_scaled = np.sort(np.abs(dct_X_test_scaled), axis=1)[:, ::-1]\n",
        "                       # Keep  top  coeff\n",
        "                       top4_dct_X_test_scaled = sorted_dct_X_test_scaled[:, :slices]\n",
        "                       #reconstruct the compressed dataset\n",
        "                       compressed_X_test_scaled = idct(top4_dct_X_test_scaled, axis=1).real\n",
        "\n",
        "                      #  print('be4',X_train_scaled[0],X_train_scaled[1])\n",
        "                      #  print('dct',dct_X_train_scaled.shape,dct_X_train_scaled[0],dct_X_train_scaled[1])\n",
        "                      #  print('comped',top4_dct_X_train_scaled[0], top4_dct_X_train_scaled[1])\n",
        "                      #  print('reconed',compressed_X_train_scaled.shape,compressed_X_train_scaled[0],compressed_X_train_scaled[1])\n",
        "\n",
        "                    elif method == 'DWT':\n",
        "\n",
        "                      # Apply DWT to X_train_scaled\n",
        "                      cA, cD = pywt.dwt(X_train_scaled, 'db1', axis=1)\n",
        "                      # Sort ROWS\n",
        "                      sorted_cA = np.sort(np.abs(cA), axis=1)[:, ::-1]\n",
        "                      # select biggest coefficients\n",
        "                      X_train_compressed = sorted_cA[:, :slices]\n",
        "                      # Reconstruct original data\n",
        "                      compressed_X_train_scaled = pywt.idwt(X_train_compressed, None, 'db1')\n",
        "\n",
        "                      # Apply DWT to X_train_scaled\n",
        "                      cA, cD = pywt.dwt(X_test_scaled, 'db1', axis=1)\n",
        "                      # Sort ROWS\n",
        "                      sorted_cA = np.sort(np.abs(cA), axis=1)[:, ::-1]\n",
        "                      # select biggest coefficients\n",
        "                      X_test_compressed = sorted_cA[:, :slices]\n",
        "                      # Reconstruct original data\n",
        "                      compressed_X_test_scaled = pywt.idwt(X_test_compressed, None, 'db1')\n",
        "\n",
        "\n",
        "                    elif method == 'PAA':\n",
        "                      # Apply PAA along the rows of the array\n",
        "                      paa = PAA(window_size = compression)\n",
        "                      compressed_X_train_scaled = paa.fit_transform(X_train_scaled)\n",
        "                      compressed_X_test_scaled = paa.fit_transform(X_test_scaled)\n",
        "\n",
        "\n",
        "                    # Model application\n",
        "                    regressor = LinearRegression()\n",
        "                    regressor.fit(compressed_X_train_scaled, y_train_scaled)\n",
        "\n",
        "                    # Generate predictions\n",
        "                    y_pred_scaled = regressor.predict(compressed_X_test_scaled)\n",
        "\n",
        "                    # Transform scaled predictions back to og scale\n",
        "                    #y_pred = sc.inverse_transform(y_pred_scaled)\n",
        "\n",
        "                    # performance (mean squared error, mean absolute error, and R-squared score)\n",
        "                    mse = mean_squared_error(y_test_scaled, y_pred_scaled)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    #mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
        "                    r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
        "\n",
        "                    # Add to list of measurements\n",
        "                    rmse_list.append(rmse)\n",
        "                    #mae_list.append(mae)\n",
        "                    r2_list.append(r2)\n",
        "\n",
        "                #store plots\n",
        "                fig = plot_confidence_interval(r2_list,method,window_size,compression,'SC') #plot function call\n",
        "                pdf.savefig(fig)\n",
        "                plt.close(fig)\n",
        "                fig2 = plot_confidence_interval(rmse_list,method,window_size ,compression,'RMSE') #plot function call\n",
        "                pdf.savefig(fig2)\n",
        "                plt.close(fig2)\n",
        "\n",
        "                # create a file name based on the method, subset index, stride, and compression\n",
        "                avg_r2 = np.mean(r2_list)\n",
        "                file_name = f\"{method}_window size({window_size})_compression({compression} - R2 SCORES).txt\"\n",
        "                # write the silhouette coefficients to file\n",
        "                with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "                              f.write(f\"Average R2 Score: {avg_r2}\\n\")\n",
        "                              content = str(r2_list)\n",
        "                              f.write(content)\n",
        "                              f.close()\n",
        "\n",
        "                avg_RMSE = np.mean(rmse_list)\n",
        "                file_name = f\"{method}_window size({window_size})_compression({compression} - RMSEs).txt\"\n",
        "                # write the silhouette coefficients to file\n",
        "                with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "                              f.write(f\"Average ARI Score: {avg_RMSE}\\n\")\n",
        "                              content = str(rmse_list)\n",
        "                              f.write(content)\n",
        "                              f.close()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "unZyfHfEnZWg"
      },
      "outputs": [],
      "source": [
        "#@title Backup\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.fft import fft, dct\n",
        "import pywt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "\n",
        "def plot_confidence_interval(scores,method,stride,compression,type):\n",
        "    # compute mean silhouette score and confidence interval\n",
        "    mean_score = np.mean(scores)\n",
        "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean_score, scale=stats.sem(scores))\n",
        "\n",
        "    # 2D array with the lower and upper bounds of the confidence interval\n",
        "    yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "    # create the figure and axis objects\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # plot the data with error bars\n",
        "    ax.plot(scores, marker='o')\n",
        "    ax.errorbar(x=range(len(scores)), y=scores, yerr=yerr, fmt='none', ecolor='r')\n",
        "\n",
        "    # set the axis labels and title\n",
        "    ax.set_xlabel('Subset of features')\n",
        "\n",
        "    if type == 'SC' :\n",
        "     ax.set_ylabel('R2 score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'R2 scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'R2 scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "\n",
        "\n",
        "    if type == 'RMSE' :\n",
        "     ax.set_ylabel('RMSE score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'RMSE scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'RMSE scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "    # return the figure object\n",
        "    return fig\n",
        "\n",
        "def paa(data, n_pieces):\n",
        "    \"\"\"\n",
        "    Piecewise Aggregate Approximation (PAA) on data.\n",
        "    \"\"\"\n",
        "    n = data.shape[0]\n",
        "    piece_length = int(np.ceil(n/n_pieces))\n",
        "    padded_data = np.pad(data, ((0, piece_length*n_pieces-n), (0,0)), mode='constant', constant_values=0)\n",
        "    pieces = padded_data.reshape(n_pieces, piece_length, -1)\n",
        "    return np.mean(pieces, axis=1)\n",
        "\n",
        "\n",
        "# Load data from file\n",
        "all_temperatures = np.loadtxt(\"/content/drive/MyDrive/temp.txt\", delimiter='\\t')\n",
        "all_humidities = np.loadtxt(\"/content/drive/MyDrive/humidity.txt\", delimiter='\\t')\n",
        "all_lights = np.loadtxt(\"/content/drive/MyDrive/light.txt\", delimiter='\\t')\n",
        "all_voltages = np.loadtxt(\"/content/drive/MyDrive/voltage.txt\", delimiter='\\t')\n",
        "\n",
        "data = all_voltages.T\n",
        "\n",
        "# window size\n",
        "window_sizes = [16, 32, 64, 128]\n",
        "compressions = [4, 8, 16]\n",
        "methods = ['DFT', 'DCT', 'DWT', 'PAA']\n",
        "\n",
        "#file for storing\n",
        "folder_name = f\"Linear Regression voltages  Measurements\"\n",
        "if not os.path.exists(folder_name):\n",
        "   os.makedirs(folder_name)\n",
        "\n",
        "raw_folder_name = f\"RAW DATA\"\n",
        "if not os.path.exists(os.path.join(folder_name,raw_folder_name)):\n",
        "   os.makedirs(os.path.join(folder_name,raw_folder_name))\n",
        "pdf_file = os.path.join(folder_name,raw_folder_name, 'plots.pdf')\n",
        "\n",
        "with PdfPages(pdf_file, 'a') as pdf:\n",
        "  for window_size in window_sizes:\n",
        "\n",
        "     # lists to store metrics for each window\n",
        "     rmse_list = []\n",
        "     #mae_list = []\n",
        "     r2_list = []\n",
        "\n",
        "     for i in range(0, data.shape[1]-window_size*2+1, window_size):\n",
        "          # Split data\n",
        "          X_train, X_test, y_train, y_test = train_test_split(data[:, i:i+window_size], data[:, i+window_size:i+window_size*2], test_size=0.25, random_state=13)\n",
        "          #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "          # Data scaling\n",
        "          sc = MinMaxScaler()\n",
        "          X_train_scaled = sc.fit_transform(X_train)\n",
        "          X_test_scaled = sc.transform(X_test)\n",
        "          y_train_scaled = sc.fit_transform(y_train.reshape(y_train.shape[0],-1))\n",
        "          y_test_scaled = sc.transform(y_test.reshape(y_test.shape[0],-1))\n",
        "\n",
        "          # Model application\n",
        "          regressor = LinearRegression()\n",
        "          regressor.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "          # Generate predictions\n",
        "          y_pred_scaled = regressor.predict(X_test_scaled)\n",
        "\n",
        "          # performance (mean squared error, mean absolute error, and R-squared score)\n",
        "          mse = mean_squared_error(y_test_scaled, y_pred_scaled)\n",
        "          rmse = np.sqrt(mse)\n",
        "          #mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
        "          r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
        "          rmse_list.append(rmse)\n",
        "          r2_list.append(r2)\n",
        "\n",
        "     #store plots\n",
        "     methd = 'RAW'\n",
        "     comprsn = 0\n",
        "     fig = plot_confidence_interval(r2_list,methd,window_size ,comprsn,'SC') #plot function call\n",
        "     pdf.savefig(fig)\n",
        "     plt.close(fig)\n",
        "\n",
        "     fig2 = plot_confidence_interval(rmse_list,methd,window_size ,comprsn,'RMSE') #plot function call\n",
        "     pdf.savefig(fig2)\n",
        "     plt.close(fig2)\n",
        "\n",
        "     # create a file name based on the method, subset index, stride, and compression\n",
        "     avg_r2 = np.mean(r2_list)\n",
        "     file_name = f\"Window size({window_size}) - R2-scores.txt\"\n",
        "     with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Average R2 Score: {avg_r2}\\n\")\n",
        "              content = str(r2_list)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "     avg_rmse = np.mean(rmse_list)\n",
        "     file_name = f\"Window size({window_size}) - RMSE scores).txt\"\n",
        "     with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Average RMSE Score: {avg_rmse}\\n\")\n",
        "              content = str(rmse_list)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "\n",
        "for window_size in window_sizes:\n",
        "\n",
        "    #file for storing\n",
        "    stride_folder_name = f\"stride({window_size})\"\n",
        "    if not os.path.exists(os.path.join(folder_name,stride_folder_name)):\n",
        "        os.makedirs(os.path.join(folder_name,stride_folder_name))\n",
        "\n",
        "    for compression in compressions:\n",
        "\n",
        "        #files for storing measurements\n",
        "        compression_folder_name = f\"compression({compression})\"\n",
        "        if not os.path.exists(os.path.join(folder_name,stride_folder_name, compression_folder_name)):\n",
        "            os.makedirs(os.path.join(folder_name,stride_folder_name, compression_folder_name))\n",
        "\n",
        "        #files for storing plots\n",
        "        plots_dir = os.path.join(folder_name,stride_folder_name, compression_folder_name, 'plots')\n",
        "        os.makedirs(plots_dir, exist_ok=True)\n",
        "        pdf_file = os.path.join(plots_dir, 'plots.pdf')\n",
        "\n",
        "        with PdfPages(pdf_file, 'a') as pdf:\n",
        "            for method in methods:\n",
        "\n",
        "                slices = window_size // compression\n",
        "\n",
        "                # lists to store metrics for each window\n",
        "                rmse_list = []\n",
        "                #mae_list = []\n",
        "                r2_list = []\n",
        "\n",
        "\n",
        "                # Iterate over the windows\n",
        "                for i in range(0, data.shape[1]-window_size*2+1, window_size):\n",
        "\n",
        "                    # Split data\n",
        "                    X_train, X_test, y_train, y_test = train_test_split(data[:, i:i+window_size], data[:, i+window_size:i+window_size*2], test_size=0.25, random_state=13)\n",
        "                    #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "                    # Data scaling\n",
        "                    sc = MinMaxScaler()\n",
        "                    X_train_scaled = sc.fit_transform(X_train)\n",
        "                    X_test_scaled = sc.transform(X_test)\n",
        "                    y_train_scaled = sc.fit_transform(y_train.reshape(y_train.shape[0],-1))\n",
        "                    y_test_scaled = sc.transform(y_test.reshape(y_test.shape[0],-1))\n",
        "\n",
        "                    if method == 'DFT':\n",
        "\n",
        "                       X_train_scaled = np.abs(fft(X_train_scaled.T, axis=0))\n",
        "                       X_train_scaled  = (np.sort(X_train_scaled , axis=0)[::-1][:slices]).T\n",
        "\n",
        "                       X_test_scaled = np.abs(fft(X_test_scaled.T, axis=0))\n",
        "                       X_test_scaled= (np.sort(X_test_scaled, axis=0)[::-1][:slices]).T\n",
        "\n",
        "                       y_train_scaled = np.abs(fft(y_train_scaled.T, axis=0))\n",
        "                       y_train_scaled = (np.sort(y_train_scaled, axis=0)[::-1][:slices]).T\n",
        "\n",
        "                       y_test_scaled = np.abs(fft(y_test_scaled.T, axis=0))\n",
        "                       y_test_scaled = (np.sort(y_test_scaled, axis=0)[::-1][:slices]).T\n",
        "                      # print(X_train_scaled .shape, X_test_scaled.shape, y_train_scaled.shape, y_test_scaled.shape)\n",
        "\n",
        "                    elif method == 'DCT':\n",
        "\n",
        "                      X_train_scaled = dct(X_train_scaled.T, axis=0)\n",
        "                      X_test_scaled = dct(X_test_scaled.T, axis=0)\n",
        "                      y_train_scaled = dct(y_train_scaled.T, axis=0)\n",
        "                      y_test_scaled = dct(y_test_scaled.T, axis=0)\n",
        "\n",
        "                      X_train_scaled  = (np.sort(X_train_scaled , axis=0)[::-1][:slices]).T\n",
        "                      X_test_scaled= (np.sort(X_test_scaled, axis=0)[::-1][:slices]).T\n",
        "                      y_train_scaled = (np.sort(y_train_scaled, axis=0)[::-1][:slices]).T\n",
        "                      y_test_scaled = (np.sort(y_test_scaled, axis=0)[::-1][:slices]).T\n",
        "\n",
        "                    elif method == 'DWT':\n",
        "\n",
        "                      cA, cD = pywt.dwt(X_train_scaled.T, 'db1', axis=0)\n",
        "                      X_train_scaled = np.concatenate((cA, cD), axis=0)\n",
        "                      X_train_scaled = (np.sort(X_train_scaled, axis=0)[::-1][:slices]).T\n",
        "\n",
        "                      cA, cD = pywt.dwt(X_test_scaled.T, 'db1', axis=0)\n",
        "                      X_test_scaled = np.concatenate((cA, cD), axis=0)\n",
        "                      X_test_scaled = (np.sort(X_test_scaled, axis=0)[::-1][:slices]).T\n",
        "\n",
        "                      cA, cD = pywt.dwt(y_train_scaled.T, 'db1', axis=0)\n",
        "                      y_train_scaled = np.concatenate((cA, cD), axis=0)\n",
        "                      y_train_scaled = (np.sort(y_train_scaled, axis=0)[::-1][:slices]).T\n",
        "\n",
        "                      cA, cD = pywt.dwt(y_test_scaled.T, 'db1', axis=0)\n",
        "                      y_test_scaled = np.concatenate((cA, cD), axis=0)\n",
        "                      y_test_scaled = (np.sort(y_test_scaled, axis=0)[::-1][:slices]).T\n",
        "                      print('DWT=' , X_train_scaled .shape, X_test_scaled.shape, y_train_scaled.shape, y_test_scaled.shape)\n",
        "\n",
        "                    elif method == 'PAA':\n",
        "\n",
        "                      X_train_scaled = paa(X_train_scaled.T, n_pieces=slices)\n",
        "                      X_train_scaled = X_train_scaled.T\n",
        "\n",
        "                      X_test_scaled = paa(X_test_scaled.T, n_pieces=slices)\n",
        "                      X_test_scaled = X_test_scaled.T\n",
        "\n",
        "                      y_train_scaled = paa(y_train_scaled.T, n_pieces=slices)\n",
        "                      y_train_scaled = y_train_scaled.T\n",
        "\n",
        "                      y_test_scaled = paa( y_test_scaled.T, n_pieces=slices)\n",
        "                      y_test_scaled =  y_test_scaled.T\n",
        "                      print('PAA =' , X_train_scaled .shape, X_test_scaled.shape, y_train_scaled.shape, y_test_scaled.shape)\n",
        "\n",
        "\n",
        "                    # Model application\n",
        "                    regressor = LinearRegression()\n",
        "                    regressor.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "                    # Generate predictions\n",
        "                    y_pred_scaled = regressor.predict(X_test_scaled)\n",
        "\n",
        "                    # Transform scaled predictions back to og scale\n",
        "                    #y_pred = sc.inverse_transform(y_pred_scaled)\n",
        "\n",
        "                    # performance (mean squared error, mean absolute error, and R-squared score)\n",
        "                    mse = mean_squared_error(y_test_scaled, y_pred_scaled)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    #mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
        "                    r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
        "\n",
        "                    # Add to list of measurements\n",
        "                    rmse_list.append(rmse)\n",
        "                    #mae_list.append(mae)\n",
        "                    r2_list.append(r2)\n",
        "\n",
        "                #store plots\n",
        "                fig = plot_confidence_interval(r2_list,method,window_size,compression,'SC') #plot function call\n",
        "                pdf.savefig(fig)\n",
        "                plt.close(fig)\n",
        "                fig2 = plot_confidence_interval(rmse_list,method,window_size ,compression,'RMSE') #plot function call\n",
        "                pdf.savefig(fig2)\n",
        "                plt.close(fig2)\n",
        "\n",
        "                # create a file name based on the method, subset index, stride, and compression\n",
        "                avg_r2 = np.mean(r2_list)\n",
        "                file_name = f\"{method}_window size({window_size})_compression({compression} - R2 SCORES).txt\"\n",
        "                # write the silhouette coefficients to file\n",
        "                with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "                              f.write(f\"Average R2 Score: {avg_r2}\\n\")\n",
        "                              content = str(r2_list)\n",
        "                              f.write(content)\n",
        "                              f.close()\n",
        "\n",
        "                avg_RMSE = np.mean(rmse_list)\n",
        "                file_name = f\"{method}_window size({window_size})_compression({compression} - RMSEs).txt\"\n",
        "                # write the silhouette coefficients to file\n",
        "                with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "                              f.write(f\"Average ARI Score: {avg_RMSE}\\n\")\n",
        "                              content = str(rmse_list)\n",
        "                              f.write(content)\n",
        "                              f.close()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YPc2fPOikN6y"
      },
      "outputs": [],
      "source": [
        "#@title Pred for window xoris 4/8/16 compression\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.fft import fft, dct\n",
        "import pywt\n",
        "\n",
        "def paa(data, n_pieces):\n",
        "    \"\"\"\n",
        "    Piecewise Aggregate Approximation (PAA) on data.\n",
        "    \"\"\"\n",
        "    n = data.shape[0]\n",
        "    piece_length = int(np.ceil(n/n_pieces))\n",
        "    padded_data = np.pad(data, ((0, piece_length*n_pieces-n), (0,0)), mode='constant', constant_values=0)\n",
        "    pieces = padded_data.reshape(n_pieces, piece_length, -1)\n",
        "    return np.mean(pieces, axis=1)\n",
        "\n",
        "\n",
        "# Load data from file\n",
        "all_temperatures = np.loadtxt(\"/content/drive/MyDrive/temp.txt\", delimiter='\\t')\n",
        "all_humidities = np.loadtxt(\"/content/drive/MyDrive/humidity.txt\", delimiter='\\t')\n",
        "all_lights = np.loadtxt(\"/content/drive/MyDrive/light.txt\", delimiter='\\t')\n",
        "all_voltages = np.loadtxt(\"/content/drive/MyDrive/voltage.txt\", delimiter='\\t')\n",
        "\n",
        "data = all_voltages.T\n",
        "\n",
        "# window size\n",
        "window_size = 128 #@param {type:\"number\"}\n",
        "method = 4              #@param {type:\"number\"} 1=DFT , 2=DCT, 3=discrete wavelet transform , 4=PAA\n",
        "\n",
        "# lists to store metrics for each window\n",
        "mse_list = []\n",
        "mae_list = []\n",
        "r2_list = []\n",
        "subset_number = 1\n",
        "\n",
        "\n",
        "# Iterate over the windows\n",
        "for i in range(0, data.shape[1]-window_size*2+1, window_size):\n",
        "\n",
        "  # Split data\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data[:, i:i+window_size], data[:, i+window_size:i+window_size*2], test_size=0.25, random_state=13)\n",
        "  #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "  # Data scaling\n",
        "  sc = MinMaxScaler()\n",
        "  X_train_scaled = sc.fit_transform(X_train)\n",
        "  X_test_scaled = sc.transform(X_test)\n",
        "  y_train_scaled = sc.fit_transform(y_train.reshape(y_train.shape[0],-1))\n",
        "  y_test_scaled = sc.transform(y_test.reshape(y_test.shape[0],-1))\n",
        "\n",
        "  if method == 1:\n",
        "        X_train_scaled = np.abs(fft(X_train_scaled, axis=0))\n",
        "        X_test_scaled = np.abs(fft(X_test_scaled, axis=0))\n",
        "        y_train_scaled = np.abs(fft(y_train_scaled, axis=0))\n",
        "        y_test_scaled = np.abs(fft(y_test_scaled, axis=0))\n",
        "        print_string = 'DFT'\n",
        "  elif method == 2:\n",
        "        X_train_scaled = dct(X_train_scaled, axis=0)\n",
        "        X_test_scaled = dct(X_test_scaled, axis=0)\n",
        "        y_train_scaled = dct(y_train_scaled, axis=0)\n",
        "        y_test_scaled = dct( y_test_scaled, axis=0)\n",
        "        print_string = 'DCT'\n",
        "  elif method == 3:\n",
        "        cA, cD = pywt.dwt(X_train_scaled, 'db1', axis=0)\n",
        "        X_train_scaled = np.concatenate((cA, cD), axis=0)\n",
        "        cA, cD = pywt.dwt(X_test_scaled, 'db1', axis=0)\n",
        "        X_test_scaled = np.concatenate((cA, cD), axis=0)\n",
        "        cA, cD = pywt.dwt(y_train_scaled, 'db1', axis=0)\n",
        "        y_train_scaled = np.concatenate((cA, cD), axis=0)\n",
        "        cA, cD = pywt.dwt(y_test_scaled, 'db1', axis=0)\n",
        "        y_test_scaled = np.concatenate((cA, cD), axis=0)\n",
        "        print_string = 'DWT'\n",
        "  elif method == 4:\n",
        "        X_train_scaled = paa(X_train_scaled, n_pieces=8)\n",
        "        X_test_scaled = paa(X_test_scaled, n_pieces=8)\n",
        "        y_train_scaled = paa(y_train_scaled, n_pieces=8)\n",
        "        y_test_scaled = paa( y_test_scaled, n_pieces=8)\n",
        "        print_string = 'PAA'\n",
        "  else   :\n",
        "      print_string = 'RAW'\n",
        "  # Model application\n",
        "  regressor = LinearRegression()\n",
        "  regressor.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "  # Generate predictions\n",
        "  y_pred_scaled = regressor.predict(X_test_scaled)\n",
        "\n",
        "  # Transform scaled predictions back to og scale\n",
        "  #y_pred = sc.inverse_transform(y_pred_scaled)\n",
        "\n",
        "  # performance (mean squared error, mean absolute error, and R-squared score)\n",
        "  mse = mean_squared_error(y_test_scaled, y_pred_scaled)\n",
        "  mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
        "  r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
        "\n",
        "  print(f\"Subset {subset_number} : | MSE= {mse:.5f}, | MAE= {mae:.5f}, R-squared= {r2:.5f}\")\n",
        "  subset_number= subset_number + 1\n",
        "  # Add to list of measurements\n",
        "  mse_list.append(mse)\n",
        "  mae_list.append(mae)\n",
        "  r2_list.append(r2)\n",
        "\n",
        "# Saving the scores array in a text file\n",
        "file_name = f\"Linear Regression({window_size}) {print_string} measurements.txt\"\n",
        "file = open(file_name, \"w+\")\n",
        "content = str(r2_list)\n",
        "file.write(content)\n",
        "file.close()\n",
        "\n",
        "  #plot\n",
        "\n",
        "# mean silhouette score and confidence interval\n",
        "mean_score = np.mean(r2_list)\n",
        "ci = stats.t.interval(0.95, len(r2_list)-1, loc=mean_score, scale=stats.sem(r2_list))\n",
        "\n",
        "# 2D array with the lower and upper bounds of the confidence interval\n",
        "yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "# Plot the data with error bars\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(r2_list, marker='o')\n",
        "ax.errorbar(x=range(len(r2_list)), y=r2_list, yerr=yerr, fmt='none', ecolor='r')\n",
        "ax.set_xlabel('Subset of features')\n",
        "ax.set_ylabel('R2 score')\n",
        "ax.set_title('R2 scores with 95% confidence interval')\n",
        "plt.show()\n",
        "\n",
        "print(f\"95% confidence interval: [{ci[0]:.5f}, {ci[1]:.5f}]\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "D8Ai3MATX-dN"
      },
      "outputs": [],
      "source": [
        "#@title Pred mono gia 1 temp\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load data from file\n",
        "all_temperatures = np.loadtxt(\"/content/drive/MyDrive/temp.txt\", delimiter='\\t')\n",
        "#all_humidities = np.loadtxt(\"/content/drive/MyDrive/humidity.txt\", delimiter='\\t')\n",
        "#all_lights = np.loadtxt(\"/content/drive/MyDrive/light.txt\", delimiter='\\t')\n",
        "#all_voltages = np.loadtxt(\"/content/drive/MyDrive/voltage.txt\", delimiter='\\t')\n",
        "\n",
        "data = all_temperatures.T\n",
        "\n",
        "# window size\n",
        "window_size = 16 #param {type:\"number\"}\n",
        "\n",
        "# lists to store metrics for each window\n",
        "mse_list = []\n",
        "mae_list = []\n",
        "r2_list = []\n",
        "subset_number = 1\n",
        "\n",
        "# Iterate over the windows\n",
        "for i in range(0, data.shape[1]-window_size+1, window_size):\n",
        "\n",
        "  # Split data\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data[:, i:i+window_size-1], data[:, i+window_size], test_size=0.25, random_state=13)\n",
        "  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "  # Data scaling\n",
        "  sc = MinMaxScaler()\n",
        "  X_train_scaled = sc.fit_transform(X_train)\n",
        "  X_test_scaled = sc.transform(X_test)\n",
        "  y_train_scaled = sc.fit_transform(y_train.reshape(y_train.shape[0],1))\n",
        "  y_test_scaled = sc.transform(y_test.reshape(y_test.shape[0],1))\n",
        "\n",
        "  # Model application\n",
        "  regressor = LinearRegression()\n",
        "  regressor.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "  # Generate predictions\n",
        "  y_pred_scaled = regressor.predict(X_test_scaled)\n",
        "\n",
        "  # Transform scaled predictions back to og scale\n",
        "  y_pred = sc.inverse_transform(y_pred_scaled)\n",
        "\n",
        "  # performance (mean squared error, mean absolute error, and R-squared score)\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "  print(f\"Subset {subset_number} : | MSE= {mse:.5f}, | MAE= {mae:.5f}, R-squared= {r2:.5f}\")\n",
        "  subset_number= subset_number + 1\n",
        "  # Add to list of measurements\n",
        "  mse_list.append(mse)\n",
        "  mae_list.append(mae)\n",
        "  r2_list.append(r2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qyfiFoufMKMG"
      },
      "outputs": [],
      "source": [
        "#@title Linear for entire fle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data from file\n",
        "\n",
        "all_temperatures = np.loadtxt(\"/content/drive/MyDrive/temp.txt\", delimiter='\\t')\n",
        "#all_humidities = np.loadtxt(\"/content/drive/MyDrive/humidity.txt\", delimiter='\\t')\n",
        "#all_lights = np.loadtxt(\"/content/drive/MyDrive/light.txt\", delimiter='\\t')\n",
        "#all_voltages = np.loadtxt(\"/content/drive/MyDrive/voltage.txt\", delimiter='\\t')\n",
        "\n",
        "data = all_temperatures.T\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data [:,:-1], data[:,-1], test_size=0.25, random_state=13)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "# Data scaling\n",
        "sc = MinMaxScaler()\n",
        "X_train_scaled = sc.fit_transform (X_train)\n",
        "X_test_scaled = sc.transform (X_test)\n",
        "\n",
        "y_train_scaled = sc.fit_transform (y_train.reshape(y_train.shape[0],1))\n",
        "y_test_scaled = sc.transform (y_test.reshape(y_test.shape[0],1))\n",
        "\n",
        "# Model application\n",
        "reg_line = LinearRegression()\n",
        "reg_line.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Generate predictions\n",
        "y_pred_scaled = reg_line.predict(X_test_scaled)\n",
        "\n",
        "# Transform scaled predictions back to og scale\n",
        "y_pred = sc.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# performance (mean squared error, mean absolute error, and R-squared score)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE: \", mse)\n",
        "print(\"MAE: \", mae)\n",
        "print(\"R-squared: \", r2)\n",
        "\n",
        "\n",
        "\n",
        "# plot the predicted values against the actual values\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"Actual values\")\n",
        "plt.ylabel(\"Predicted values\")\n",
        "plt.title(\"Linear Regression Results\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}