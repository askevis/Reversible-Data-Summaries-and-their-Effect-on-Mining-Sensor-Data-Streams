{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyts\n",
        "!pip install --upgrade pyts"
      ],
      "metadata": {
        "id": "WXS2Rwdt9k76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e781084-7a46-4f23-a2b9-e89e58172c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyts\n",
            "  Downloading pyts-0.13.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.3.2)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.10/dist-packages (from pyts) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.2->pyts) (0.41.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->pyts) (3.2.0)\n",
            "Installing collected packages: pyts\n",
            "Successfully installed pyts-0.13.0\n",
            "Requirement already satisfied: pyts in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from pyts) (1.3.2)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.10/dist-packages (from pyts) (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.2->pyts) (0.41.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->pyts) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "import sys\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.fft import fft, dct\n",
        "import pywt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from pyts.approximation import PiecewiseAggregateApproximation as PAA\n",
        "from scipy.fftpack import idct\n",
        "\n",
        "def generate_random_vectors(n, r):\n",
        "    random_vectors = np.random.normal(0, 1, size=(r, n))\n",
        "    return random_vectors\n",
        "\n",
        "def MINE(data,R,R_inv):\n",
        "\n",
        "  prod = np.matmul(data,R)\n",
        "  bitmap = np.where(prod < 0, 0, 1)\n",
        "  recon = np.matmul(bitmap,R_inv)\n",
        "  return recon\n",
        "\n",
        "\n",
        "def plot_confidence_interval(scores,method,stride,compression,type):\n",
        "    # compute mean silhouette score and confidence interval\n",
        "    mean_score = np.mean(scores)\n",
        "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean_score, scale=stats.sem(scores))\n",
        "\n",
        "    # 2D array with the lower and upper bounds of the confidence interval\n",
        "    yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "    # create the figure and axis objects\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # plot the data with error bars\n",
        "    ax.plot(scores, marker='o')\n",
        "    ax.errorbar(x=range(len(scores)), y=scores, yerr=yerr, fmt='none', ecolor='r')\n",
        "\n",
        "    # set the axis labels and title\n",
        "    ax.set_xlabel('Number of windows')\n",
        "\n",
        "    if type == 'SC' :\n",
        "     ax.set_ylabel('Silhouette score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'Silhouette scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'Silhouette scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "\n",
        "\n",
        "    if type == 'ARI' :\n",
        "     ax.set_ylabel('ARI score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'ARI scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'ARI scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "    # return the figure object\n",
        "    return fig\n",
        "\n",
        "def plot_score_confidence_interval(data,window_size,compression):\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))  # Adjust the figure size as needed\n",
        "\n",
        "    colors = ['b', 'g', 'r', 'c', 'm']  # Colors for each method\n",
        "\n",
        "    y_axis_limits = (-1.5, 1.5)  # Outlier limits\n",
        "\n",
        "    for i, (scores, method) in enumerate(data):\n",
        "        ax = axs[i]\n",
        "        n = len(scores)\n",
        "        x = np.arange(n) + 1\n",
        "        y = np.array(scores)\n",
        "\n",
        "        # Identify and count outliers based on the limits\n",
        "        outliers = np.where((y < -1.5) | (y > 1.5))[0]\n",
        "        num_outliers = len(outliers)\n",
        "\n",
        "        # Exclude outliers from the data\n",
        "        filtered_scores = np.delete(y, outliers)\n",
        "        filtered_x = np.delete(x, outliers)\n",
        "\n",
        "        mean = np.mean(filtered_scores)\n",
        "        # Calculate the confidence interval\n",
        "        ci = stats.t.interval(0.95, len(filtered_scores), loc=mean, scale=stats.sem(filtered_scores))\n",
        "        # Plot the non-outlier data points and mean\n",
        "        ax.plot(filtered_x, filtered_scores, 'o', markersize=4, color=colors[i])\n",
        "        ax.plot(x, [mean] * n, '--', color=colors[i])\n",
        "\n",
        "        fig.suptitle(f'Silhouette scores with 95% confidence interval // W= {window_size}, C= {compression} ')\n",
        "\n",
        "        ax.set_xlabel('Window')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title(f'{method} : [{ci[0]:.5f}, {ci[1]:.5f}] ')\n",
        "        ax.grid(True)\n",
        "\n",
        "        # Set the y-axis limits to the predefined range\n",
        "        ax.set_ylim(y_axis_limits)\n",
        "\n",
        "        # Add the number of outliers as text in the top right corner\n",
        "        ax.text(0.85, 0.9, f'Outliers: {num_outliers}', transform=ax.transAxes, fontsize=10, color='red', ha='right')\n",
        "\n",
        "    return fig\n",
        "\n",
        "def plot_ARI_confidence_interval(data,window_size,compression):\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))  # Adjust the figure size as needed\n",
        "\n",
        "    colors = ['b', 'g', 'r', 'c', 'm']  # Colors for each method\n",
        "    y_axis_limits = (-1.5, 1.5) # Outlier limits\n",
        "\n",
        "    for i, (scores, method) in enumerate(data):\n",
        "        ax = axs[i]\n",
        "        n = len(scores)\n",
        "        x = np.arange(n) + 1\n",
        "        y = np.array(scores)\n",
        "        # Identify and count outliers based on the limits\n",
        "        outliers = np.where((y < -1.5) | (y > 1.5))[0]\n",
        "        num_outliers = len(outliers)\n",
        "\n",
        "        # Exclude outliers from the data\n",
        "        filtered_scores = np.delete(y, outliers)\n",
        "        filtered_x = np.delete(x, outliers)\n",
        "\n",
        "        mean = np.mean(filtered_scores)\n",
        "        # Calculate the confidence interval\n",
        "        ci = stats.t.interval(0.95, len(filtered_scores), loc=mean, scale=stats.sem(filtered_scores))\n",
        "\n",
        "        # Plot the non-outlier data points and mean\n",
        "        ax.plot(filtered_x, filtered_scores, 'o', markersize=4, color=colors[i])\n",
        "        ax.plot(x, [mean] * n, '--', color=colors[i])\n",
        "\n",
        "        fig.suptitle(f'ARI scores with 95% confidence interval // W= {window_size}, C= {compression} ')\n",
        "\n",
        "        ax.set_xlabel('Window')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title(f'{method} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "        ax.grid(True)\n",
        "\n",
        "        # Set the y-axis limits to the predefined range\n",
        "        ax.set_ylim(y_axis_limits)\n",
        "\n",
        "        # Add the number of outliers as text in the top right corner\n",
        "        ax.text(0.85, 0.9, f'Outliers: {num_outliers}', transform=ax.transAxes, fontsize=10, color='red', ha='right')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/pump_sensor2.txt', delimiter=',')\n",
        "\n",
        "# BROKEN,NORMAL,RECOVERING -> 0,0.5,1\n",
        "le = LabelEncoder()\n",
        "df.iloc[:, -1] = le.fit_transform(df.iloc[:, -1])\n",
        "max_label = df.iloc[:, -1].max()\n",
        "df.iloc[:, -1] = df.iloc[:, -1] / max_label * 1.0\n",
        "# Discard the first row + first two columns\n",
        "df = df.iloc[:, 2:]\n",
        "data = df.to_numpy(dtype='float')\n",
        "# Split  features (X) and target (y)\n",
        "X = data[:, :-1]\n",
        "#print(X)\n",
        "y = data[:, -1]\n",
        "#print(y[0])\n",
        "# Fill Nan values based on the mean of each sensor\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "# Select 1000 0.5 kai 1 labels\n",
        "label_0_indices = np.where(y == 0)[0]\n",
        "label_05_indices = np.where(y == 0.5)[0]\n",
        "label_1_indices = np.where(y == 1)[0]\n",
        "label_05_indices = np.random.choice(label_05_indices, size=1000, replace=False)\n",
        "label_1_indices = np.random.choice(label_1_indices, size=1000, replace=False)\n",
        "# Concatenate + shuffle\n",
        "indices = np.concatenate((label_0_indices, label_05_indices, label_1_indices))\n",
        "np.random.seed(123)   #Gia idio dataset shuffle se ola ta algorithms\n",
        "np.random.shuffle(indices)\n",
        "data = X[indices]\n",
        "data = data.T\n",
        "\n",
        "# Data scaling\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Params\n",
        "n_features = scaled_data.shape[1]\n",
        "strides = [16, 32, 64, 128]\n",
        "compressions = [4, 8, 16]\n",
        "methods = ['DFT', 'DCT', 'DWT', 'PAA','MINE']\n",
        "all_labels = []  # create an empty list to store labels for all strides-ARI\n",
        "\n",
        "#file for storing\n",
        "folder_name = f\"DBSCAN new PUMP Measurements\"\n",
        "if not os.path.exists(folder_name):\n",
        "   os.makedirs(folder_name)\n",
        "\n",
        "raw_folder_name = f\"RAW DATA\"\n",
        "if not os.path.exists(os.path.join(folder_name,raw_folder_name)):\n",
        "   os.makedirs(os.path.join(folder_name,raw_folder_name))\n",
        "\n",
        "pdf_file = os.path.join(folder_name,raw_folder_name, 'plots.pdf')\n",
        "\n",
        "with PdfPages(pdf_file, 'a') as pdf:\n",
        " for stride in strides:\n",
        "\n",
        "     #set appropriate eps based on elbow crit.\n",
        "     if stride == 16 : eps = 0.15\n",
        "     elif stride == 32 : eps = 0.26\n",
        "     elif stride == 64 : eps = 0.3\n",
        "     elif stride == 128 : eps = 1\n",
        "\n",
        "     silhouette_scores = []\n",
        "     bad_clusters = 0  # count subsets with zero clusters\n",
        "     all_zero_clusters = 0\n",
        "     subset_num = 0  #to accurately store labels for ARI\n",
        "\n",
        "     for i in range(0, n_features-stride, stride):\n",
        "\n",
        "      subset_length = min(stride, n_features-i)\n",
        "      subset_data = scaled_data[:, i:i+subset_length]\n",
        "      clustering = DBSCAN(eps=eps, min_samples=5).fit(subset_data)\n",
        "\n",
        "      # cluster results\n",
        "      labels = clustering.labels_\n",
        "      all_labels.append((stride, subset_num, labels))  # append the labels for this stride to all_labels\n",
        "      n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "      if len(set(labels)) > 1:\n",
        "        score = silhouette_score(subset_data , clustering.labels_)\n",
        "        silhouette_scores.append(score)\n",
        "      else:\n",
        "        bad_clusters += 1\n",
        "      # Count number of zero clusters\n",
        "      if  np.all(labels == 0):\n",
        "             all_zero_clusters += 1\n",
        "             silhouette_scores.append(0)\n",
        "      subset_num = subset_num + 1\n",
        "\n",
        "     #store plots\n",
        "     methd = 'RAW'\n",
        "     comprsn = 0\n",
        "     fig = plot_confidence_interval(silhouette_scores,methd,stride,comprsn,'SC') #plot function call\n",
        "     pdf.savefig(fig)\n",
        "     plt.close(fig)\n",
        "\n",
        "     # check an exo toulaxiston ena score - avoid errors\n",
        "     if silhouette_scores:\n",
        "       avg_silhouette = np.mean(silhouette_scores)\n",
        "     else:\n",
        "       avg_silhouette = None\n",
        "     # create a file name based on the method, subset index, stride, and compression\n",
        "     avg_silhouette = np.mean(silhouette_scores)\n",
        "     file_name = f\"RAW DATA_stride({stride}).txt\"\n",
        "     # write the silhouette coefficients to file\n",
        "     with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "      f.write(f\"Number of subsets where clustering was not possible: {bad_clusters}\\n\")\n",
        "      f.write(f\"Number of clusters filled with only zeros: {all_zero_clusters}\\n\")\n",
        "      f.write(f\"Average Silhouette Score: {avg_silhouette}\\n\")\n",
        "      content = str(silhouette_scores)\n",
        "      f.write(content)\n",
        "      f.close()\n",
        "\n",
        "for stride in strides:\n",
        "    #set appropriate eps based on elbow crit.\n",
        "    if stride == 16 : eps = 0.15\n",
        "    elif stride == 32 : eps = 0.26\n",
        "    elif stride == 64 : eps = 0.3\n",
        "    elif stride == 128 : eps = 0.4\n",
        "\n",
        "    #file for storing\n",
        "    stride_folder_name = f\"stride({stride}),eps = {eps})\"\n",
        "    if not os.path.exists(os.path.join(folder_name,stride_folder_name)):\n",
        "        os.makedirs(os.path.join(folder_name,stride_folder_name))\n",
        "\n",
        "    for compression in compressions:\n",
        "\n",
        "        #vector and inverse vector arrays for our algorithm\n",
        "        n = 64*int(stride/compression)  # Dimension of each random vector\n",
        "        r = stride  # Number of random vectors to generate/rows\n",
        "        R = generate_random_vectors(n, r)\n",
        "        R_inv= np.linalg.pinv(R)\n",
        "\n",
        "        #files for storing measurements\n",
        "        compression_folder_name = f\"compression({compression})\"\n",
        "        if not os.path.exists(os.path.join(folder_name,stride_folder_name, compression_folder_name)):\n",
        "            os.makedirs(os.path.join(folder_name,stride_folder_name, compression_folder_name))\n",
        "\n",
        "        #files for storing plots\n",
        "        plots_dir = os.path.join(folder_name,stride_folder_name, compression_folder_name, 'plots')\n",
        "        os.makedirs(plots_dir, exist_ok=True)\n",
        "        pdf_file = os.path.join(plots_dir, 'plots.pdf')\n",
        "\n",
        "        with PdfPages(pdf_file, 'a') as pdf:\n",
        "         results1 = []\n",
        "         results2 = []\n",
        "         for method in methods:\n",
        "\n",
        "            slices = stride // compression\n",
        "            silhouette_scores = []\n",
        "            ARI_scores = []\n",
        "            bad_clusters = 0  # count subsets with zero clusters\n",
        "            all_zero_clusters = 0\n",
        "            num_subset = 0\n",
        "\n",
        "            # Iterate over subsets\n",
        "            for i in range(0, n_features-stride, stride):\n",
        "              subset_length = min(stride, n_features-i)\n",
        "              subset_data = scaled_data[:, i:i+subset_length]\n",
        "\n",
        "              #vector and inverse vector arrays for our algorithm\n",
        "              # n = 64*int(stride/compression)  # Dimension of each random vector\n",
        "              # r = stride  # Number of random vectors to generate/rows\n",
        "              # R = generate_random_vectors(n, r)\n",
        "              # R_inv= np.linalg.pinv(R)\n",
        "\n",
        "              if method == 'DFT':\n",
        "\n",
        "                       #array to store the restored data\n",
        "                       compressed_subset_data = np.zeros_like(subset_data)\n",
        "\n",
        "                       for i in range(51):\n",
        "                          # Compute abs DFT of each row\n",
        "                          dft_subset_data =  np.fft.fft(subset_data[i])\n",
        "                          # Sort the DFT coefficients along each row by their magnitude\n",
        "                          sorted_indices = np.argsort(-np.abs(dft_subset_data))[:slices]\n",
        "                          sorted_dft_subset_data = dft_subset_data[sorted_indices]\n",
        "                          # Keep  top  coeff\n",
        "                          top__dft_subset_data = sorted_dft_subset_data[:slices]\n",
        "                          #reconstruct the compressed dataset\n",
        "                          compressed_subset_data[i] = np.fft.ifft(top__dft_subset_data,stride).real\n",
        "\n",
        "              elif method == 'DCT':\n",
        "\n",
        "                      #array to store the restored data\n",
        "                       compressed_subset_data = np.zeros_like(subset_data)\n",
        "\n",
        "                       for i in range(51):\n",
        "                          # Compute abs DCT of each row\n",
        "                          dct_subset_data = dct(subset_data[i])\n",
        "                          # Sort the DCT coefficients along each row by their magnitude\n",
        "                          sorted_indices = np.argsort(-np.abs(dct_subset_data))[:slices]\n",
        "                          sorted_dct_subset_data = dct_subset_data[sorted_indices]\n",
        "                          # Keep  top  coeff\n",
        "                          top__dct_subset_data = sorted_dct_subset_data[:slices]\n",
        "                          compressed_subset_data[i] = idct(top__dct_subset_data, type=2, n=stride).real\n",
        "\n",
        "              elif method == 'DWT':\n",
        "\n",
        "                    #array to store the restored data\n",
        "                      compressed_subset_data = np.zeros_like(subset_data)\n",
        "\n",
        "                      for i in range(51):\n",
        "                          # Apply DWT to each row using 'db1' wavelet\n",
        "                          cA, cD = pywt.dwt(subset_data[i], 'db1')\n",
        "                          # Sort\n",
        "                          sorted_cD_subset_data = np.zeros_like(cD) #initialize storage array\n",
        "                          sorted_indices = np.argsort(-np.abs(cD))[:slices]\n",
        "                          sorted_cD_subset_data[sorted_indices] = cD[sorted_indices]\n",
        "                          # Perform inverse DWT to restore the row\n",
        "                          compressed_subset_data[i] = pywt.idwt(cA, sorted_cD_subset_data, 'db1')\n",
        "\n",
        "\n",
        "              elif method == 'PAA':\n",
        "\n",
        "                      # Apply PAA along the rows of the array\n",
        "                      paa = PAA(window_size = compression)\n",
        "                      compressed_subset_data = paa.fit_transform(subset_data)\n",
        "\n",
        "              elif method == 'MINE':\n",
        "\n",
        "                    #array to store the restored data\n",
        "                    compressed_subset_data = np.zeros_like(subset_data)\n",
        "\n",
        "                    for i in range(51):\n",
        "                      row = subset_data[i]\n",
        "                      # Apply my algorithm along the rows of the array\n",
        "                      compressed_subset_data[i] = MINE(row,R,R_inv)\n",
        "\n",
        "              clustering = DBSCAN(eps=eps, min_samples=5).fit(compressed_subset_data )\n",
        "              labels = clustering.labels_\n",
        "\n",
        "              # cluster results\n",
        "              if len(set(labels)) > 1:\n",
        "                  score = silhouette_score(compressed_subset_data , clustering.labels_)\n",
        "                  silhouette_scores.append(score)\n",
        "              else:\n",
        "                bad_clusters += 1\n",
        "              original_labels = [labels for strde, subst_num, labels in all_labels if strde == stride and subst_num == num_subset][0]\n",
        "              ARI = adjusted_rand_score(original_labels, labels)\n",
        "              ARI_scores.append(ARI)\n",
        "              num_subset += 1\n",
        "              # Count number of zero clusters\n",
        "              if  np.all(labels == 0):\n",
        "                       all_zero_clusters += 1\n",
        "                       silhouette_scores.append(0)\n",
        "\n",
        "            results1.append((silhouette_scores, method))\n",
        "            results2.append((ARI_scores, method))\n",
        "\n",
        "            # check an exo toulaxiston ena score - avoid errors\n",
        "            if silhouette_scores:\n",
        "             avg_silhouette = np.mean(silhouette_scores)\n",
        "            else:\n",
        "             avg_silhouette = None\n",
        "\n",
        "            # create a file name based on the method, subset index, stride, and compression\n",
        "            file_name = f\"{method}_stride({stride})_compression({compression} - SILHOUETTES).txt\"\n",
        "            # write the silhouette coefficients to file\n",
        "            with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Number of subsets where clustering was not possible: {bad_clusters}\\n\")\n",
        "              f.write(f\"Number of clusters filled with only zeros: {all_zero_clusters}\\n\")\n",
        "              f.write(f\"Average Silhouette Score: {avg_silhouette}\\n\")\n",
        "              content = str(silhouette_scores)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "            avg_ARI = np.mean(ARI_scores)\n",
        "            file_name = f\"{method}_stride({stride})_compression({compression} - ARIs).txt\"\n",
        "            # write the silhouette coefficients to file\n",
        "            with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Average ARI Score: {avg_ARI}\\n\")\n",
        "              content = str(ARI_scores)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "         #store plots\n",
        "         fig = plot_score_confidence_interval(results1,stride,compression) #plot function call\n",
        "         pdf.savefig(fig)\n",
        "         plt.close(fig)\n",
        "         fig = plot_ARI_confidence_interval(results2,stride,compression) #plot function call\n",
        "         pdf.savefig(fig)\n",
        "         plt.close(fig)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HCqO4MMPpumL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90b7848-660e-4ca0-98dc-ff3799a6f79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-7d910516a031>:161: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df.iloc[:, -1] = le.fit_transform(df.iloc[:, -1])\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Backup\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "import sys\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.fft import fft, dct\n",
        "import pywt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confidence_interval(scores,method,stride,compression,type):\n",
        "    # compute mean silhouette score and confidence interval\n",
        "    mean_score = np.mean(scores)\n",
        "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean_score, scale=stats.sem(scores))\n",
        "\n",
        "    # 2D array with the lower and upper bounds of the confidence interval\n",
        "    yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "    # create the figure and axis objects\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # plot the data with error bars\n",
        "    ax.plot(scores, marker='o')\n",
        "    ax.errorbar(x=range(len(scores)), y=scores, yerr=yerr, fmt='none', ecolor='r')\n",
        "\n",
        "    # set the axis labels and title\n",
        "    ax.set_xlabel('Subset of features')\n",
        "\n",
        "    if type == 'SC' :\n",
        "     ax.set_ylabel('Silhouette score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'Silhouette scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'Silhouette scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "\n",
        "\n",
        "    if type == 'ARI' :\n",
        "     ax.set_ylabel('ARI score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'ARI scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'ARI scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "    # return the figure object\n",
        "    return fig\n",
        "\n",
        "\n",
        "def paa(data, n_pieces):\n",
        "    \"\"\"\n",
        "    Piecewise Aggregate Approximation (PAA) on data.\n",
        "    \"\"\"\n",
        "    n = data.shape[0]\n",
        "    piece_length = int(np.ceil(n/n_pieces))\n",
        "    padded_data = np.pad(data, ((0, piece_length*n_pieces-n), (0,0)), mode='constant', constant_values=0)\n",
        "    pieces = padded_data.reshape(n_pieces, piece_length, -1)\n",
        "    return np.mean(pieces, axis=1)\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/pump_sensor2.txt', delimiter=',')\n",
        "\n",
        "# BROKEN,NORMAL,RECOVERING -> 0,0.5,1\n",
        "le = LabelEncoder()\n",
        "df.iloc[:, -1] = le.fit_transform(df.iloc[:, -1])\n",
        "max_label = df.iloc[:, -1].max()\n",
        "df.iloc[:, -1] = df.iloc[:, -1] / max_label * 1.0\n",
        "# Discard the first row + first two columns\n",
        "df = df.iloc[:, 2:]\n",
        "data = df.to_numpy(dtype='float')\n",
        "# Split  features (X) and target (y)\n",
        "X = data[:, :-1]\n",
        "#print(X)\n",
        "y = data[:, -1]\n",
        "#print(y[0])\n",
        "# Fill Nan values based on the mean of each sensor\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "# Select 1000 0.5 kai 1 labels\n",
        "label_0_indices = np.where(y == 0)[0]\n",
        "label_05_indices = np.where(y == 0.5)[0]\n",
        "label_1_indices = np.where(y == 1)[0]\n",
        "label_05_indices = np.random.choice(label_05_indices, size=1000, replace=False)\n",
        "label_1_indices = np.random.choice(label_1_indices, size=1000, replace=False)\n",
        "# Concatenate + shuffle\n",
        "indices = np.concatenate((label_0_indices, label_05_indices, label_1_indices))\n",
        "np.random.seed(123)   #Gia idio dataset shuffle se ola ta algorithms\n",
        "np.random.shuffle(indices)\n",
        "data = X[indices]\n",
        "data = data.T\n",
        "\n",
        "# Data scaling\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Params\n",
        "n_features = scaled_data.shape[1]\n",
        "strides = [16, 32, 64, 128]\n",
        "compressions = [4, 8, 16]\n",
        "methods = ['DFT', 'DCT', 'DWT', 'PAA']\n",
        "all_labels = []  # create an empty list to store labels for all strides-ARI\n",
        "\n",
        "#file for storing\n",
        "folder_name = f\"DBSCAN PUMP Measurements\"\n",
        "if not os.path.exists(folder_name):\n",
        "   os.makedirs(folder_name)\n",
        "\n",
        "raw_folder_name = f\"RAW DATA\"\n",
        "if not os.path.exists(os.path.join(folder_name,raw_folder_name)):\n",
        "   os.makedirs(os.path.join(folder_name,raw_folder_name))\n",
        "\n",
        "pdf_file = os.path.join(folder_name,raw_folder_name, 'plots.pdf')\n",
        "\n",
        "with PdfPages(pdf_file, 'a') as pdf:\n",
        " for stride in strides:\n",
        "\n",
        "     #set appropriate eps based on elbow crit.\n",
        "     if stride == 16 : eps = 0.15\n",
        "     elif stride == 32 : eps = 0.26\n",
        "     elif stride == 64 : eps = 0.3\n",
        "     elif stride == 128 : eps = 0.4\n",
        "\n",
        "     silhouette_scores = []\n",
        "     bad_clusters = 0  # count subsets with zero clusters\n",
        "     all_zero_clusters = 0\n",
        "     subset_num = 0  #to accurately store labels for ARI\n",
        "\n",
        "     for i in range(0, n_features, stride):\n",
        "\n",
        "      subset_length = min(stride, n_features-i)\n",
        "      subset_data = scaled_data[:, i:i+subset_length]\n",
        "      clustering = DBSCAN(eps=eps, min_samples=5).fit(subset_data)\n",
        "      # cluster results\n",
        "      labels = clustering.labels_\n",
        "      all_labels.append((stride, subset_num, labels))  # append the labels for this stride to all_labels\n",
        "      n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "      if len(set(labels)) > 1:\n",
        "        score = silhouette_score(subset_data , clustering.labels_)\n",
        "        silhouette_scores.append(score)\n",
        "      else:\n",
        "        bad_clusters += 1\n",
        "      # Count number of zero clusters\n",
        "      if  np.all(labels == 0):\n",
        "             all_zero_clusters += 1\n",
        "             silhouette_scores.append(0)\n",
        "      subset_num = subset_num + 1\n",
        "\n",
        "     #store plots\n",
        "     methd = 'RAW'\n",
        "     comprsn = 0\n",
        "     fig = plot_confidence_interval(silhouette_scores,methd,stride,comprsn,'SC') #plot function call\n",
        "     pdf.savefig(fig)\n",
        "     plt.close(fig)\n",
        "\n",
        "     # check an exo toulaxiston ena score - avoid errors\n",
        "     if silhouette_scores:\n",
        "       avg_silhouette = np.mean(silhouette_scores)\n",
        "     else:\n",
        "       avg_silhouette = None\n",
        "     # create a file name based on the method, subset index, stride, and compression\n",
        "     avg_silhouette = np.mean(silhouette_scores)\n",
        "     file_name = f\"RAW DATA_stride({stride}).txt\"\n",
        "     # write the silhouette coefficients to file\n",
        "     with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "      f.write(f\"Number of subsets where clustering was not possible: {bad_clusters}\\n\")\n",
        "      f.write(f\"Number of clusters filled with only zeros: {all_zero_clusters}\\n\")\n",
        "      f.write(f\"Average Silhouette Score: {avg_silhouette}\\n\")\n",
        "      content = str(silhouette_scores)\n",
        "      f.write(content)\n",
        "      f.close()\n",
        "\n",
        "for stride in strides:\n",
        "    #set appropriate eps based on elbow crit.\n",
        "    if stride == 16 : eps = 0.15\n",
        "    elif stride == 32 : eps = 0.26\n",
        "    elif stride == 64 : eps = 0.3\n",
        "    elif stride == 128 : eps = 0.4\n",
        "\n",
        "    #file for storing\n",
        "    stride_folder_name = f\"stride({stride}),eps = {eps})\"\n",
        "    if not os.path.exists(os.path.join(folder_name,stride_folder_name)):\n",
        "        os.makedirs(os.path.join(folder_name,stride_folder_name))\n",
        "\n",
        "    for compression in compressions:\n",
        "\n",
        "        #files for storing measurements\n",
        "        compression_folder_name = f\"compression({compression})\"\n",
        "        if not os.path.exists(os.path.join(folder_name,stride_folder_name, compression_folder_name)):\n",
        "            os.makedirs(os.path.join(folder_name,stride_folder_name, compression_folder_name))\n",
        "\n",
        "        #files for storing plots\n",
        "        plots_dir = os.path.join(folder_name,stride_folder_name, compression_folder_name, 'plots')\n",
        "        os.makedirs(plots_dir, exist_ok=True)\n",
        "        pdf_file = os.path.join(plots_dir, 'plots.pdf')\n",
        "\n",
        "        with PdfPages(pdf_file, 'a') as pdf:\n",
        "         for method in methods:\n",
        "\n",
        "            slices = stride // compression\n",
        "            silhouette_scores = []\n",
        "            ARI_scores = []\n",
        "            bad_clusters = 0  # count subsets with zero clusters\n",
        "            all_zero_clusters = 0\n",
        "            num_subset = 0\n",
        "\n",
        "            # Iterate over subsets\n",
        "            for i in range(0, n_features, stride):\n",
        "              subset_length = min(stride, n_features-i)\n",
        "              subset_data = scaled_data[:, i:i+subset_length]\n",
        "\n",
        "              if method == 'DFT':\n",
        "                      subset_data = subset_data.T\n",
        "                      subset_transform = np.abs(fft(subset_data, axis=0))\n",
        "                      subset_transform = np.sort(subset_transform, axis=0)[::-1][:slices]\n",
        "                      subset_transform = subset_transform.T\n",
        "              elif method == 'DCT':\n",
        "                     subset_data = subset_data.T\n",
        "                     subset_transform = dct(subset_data, axis=0)\n",
        "                     subset_transform = np.sort(subset_transform, axis=0)[::-1][:slices]\n",
        "                     subset_transform = subset_transform.T\n",
        "              elif method == 'DWT':\n",
        "                     subset_data = subset_data.T\n",
        "                     cA, cD = pywt.dwt(subset_data, 'db1', axis=0)\n",
        "                     subset_transform = np.concatenate((cA, cD), axis=0)\n",
        "                     subset_transform = np.sort(subset_transform, axis=0)[::-1][:slices]\n",
        "                     subset_transform = subset_transform.T\n",
        "              elif method == 'PAA':\n",
        "                     subset_transform = paa(subset_data.T, n_pieces=slices)\n",
        "                     subset_transform = subset_transform.T\n",
        "\n",
        "              clustering = DBSCAN(eps=eps, min_samples=5).fit(subset_transform)\n",
        "              labels = clustering.labels_\n",
        "\n",
        "              # cluster results\n",
        "              if len(set(labels)) > 1:\n",
        "                  score = silhouette_score(subset_transform , clustering.labels_)\n",
        "                  silhouette_scores.append(score)\n",
        "              else:\n",
        "                bad_clusters += 1\n",
        "              original_labels = [labels for strde, subst_num, labels in all_labels if strde == stride and subst_num == num_subset][0]\n",
        "              ARI = adjusted_rand_score(original_labels, labels)\n",
        "              ARI_scores.append(ARI)\n",
        "              num_subset += 1\n",
        "              # Count number of zero clusters\n",
        "              if  np.all(labels == 0):\n",
        "                       all_zero_clusters += 1\n",
        "                       silhouette_scores.append(0)\n",
        "            #store plots\n",
        "            fig2 = plot_confidence_interval(silhouette_scores,method,stride,compression,'SC') #plot function call\n",
        "            pdf.savefig(fig2)\n",
        "            plt.close(fig2)\n",
        "            fig = plot_confidence_interval(ARI_scores,method,stride,compression,'ARI') #plot function call\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "            # check an exo toulaxiston ena score - avoid errors\n",
        "            if silhouette_scores:\n",
        "             avg_silhouette = np.mean(silhouette_scores)\n",
        "            else:\n",
        "             avg_silhouette = None\n",
        "\n",
        "            # create a file name based on the method, subset index, stride, and compression\n",
        "            file_name = f\"{method}_stride({stride})_compression({compression} - SILHOUETTES).txt\"\n",
        "            # write the silhouette coefficients to file\n",
        "            with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Number of subsets where clustering was not possible: {bad_clusters}\\n\")\n",
        "              f.write(f\"Number of clusters filled with only zeros: {all_zero_clusters}\\n\")\n",
        "              f.write(f\"Average Silhouette Score: {avg_silhouette}\\n\")\n",
        "              content = str(silhouette_scores)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "            avg_ARI = np.mean(ARI_scores)\n",
        "            file_name = f\"{method}_stride({stride})_compression({compression} - ARIs).txt\"\n",
        "            # write the silhouette coefficients to file\n",
        "            with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Average ARI Score: {avg_ARI}\\n\")\n",
        "              content = str(ARI_scores)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CSg7jCiKaFJJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Gia olo to file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "import sys\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.fft import fft, dct\n",
        "import pywt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confidence_interval(scores,method,stride,compression,type):\n",
        "    # compute mean silhouette score and confidence interval\n",
        "    mean_score = np.mean(scores)\n",
        "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean_score, scale=stats.sem(scores))\n",
        "\n",
        "    # 2D array with the lower and upper bounds of the confidence interval\n",
        "    yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "    # create the figure and axis objects\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # plot the data with error bars\n",
        "    ax.plot(scores, marker='o')\n",
        "    ax.errorbar(x=range(len(scores)), y=scores, yerr=yerr, fmt='none', ecolor='r')\n",
        "\n",
        "    # set the axis labels and title\n",
        "    ax.set_xlabel('Subset of features')\n",
        "\n",
        "    if type == 'SC' :\n",
        "     ax.set_ylabel('Silhouette score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'Silhouette scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'Silhouette scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "\n",
        "\n",
        "    if type == 'ARI' :\n",
        "     ax.set_ylabel('ARI score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'ARI scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'ARI scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "    # return the figure object\n",
        "    return fig\n",
        "\n",
        "\n",
        "def paa(data, n_pieces):\n",
        "    \"\"\"\n",
        "    Piecewise Aggregate Approximation (PAA) on data.\n",
        "    \"\"\"\n",
        "    n = data.shape[0]\n",
        "    piece_length = int(np.ceil(n/n_pieces))\n",
        "    padded_data = np.pad(data, ((0, piece_length*n_pieces-n), (0,0)), mode='constant', constant_values=0)\n",
        "    pieces = padded_data.reshape(n_pieces, piece_length, -1)\n",
        "    return np.mean(pieces, axis=1)\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/pump_sensor2.txt', delimiter=',')\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/pump_sensor2.txt', delimiter=',')\n",
        "\n",
        "# Discard the first row, first two columns, and last column\n",
        "df = df.iloc[:, 2:-1]\n",
        "\n",
        "# Convert to a NumPy array\n",
        "data = df.to_numpy(dtype='float')\n",
        "\n",
        "# Fill Nan me vasi ton meso oro tou kathe sensor\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "data = imputer.fit_transform(data)\n",
        "data = data.T\n",
        "\n",
        "# Data scaling\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Params\n",
        "n_features = scaled_data.shape[1]\n",
        "strides = [16, 32, 64, 128]\n",
        "compressions = [4, 8, 16]\n",
        "methods = ['DFT', 'DCT', 'DWT', 'PAA']\n",
        "all_labels = []  # create an empty list to store labels for all strides-ARI\n",
        "\n",
        "\n",
        "#file for storing\n",
        "folder_name = f\"DBSCAN PUMP Measurements\"\n",
        "if not os.path.exists(folder_name):\n",
        "   os.makedirs(folder_name)\n",
        "\n",
        "raw_folder_name = f\"RAW DATA\"\n",
        "if not os.path.exists(os.path.join(folder_name,raw_folder_name)):\n",
        "   os.makedirs(os.path.join(folder_name,raw_folder_name))\n",
        "\n",
        "pdf_file = os.path.join(folder_name,raw_folder_name, 'plots.pdf')\n",
        "\n",
        "with PdfPages(pdf_file, 'a') as pdf:\n",
        " for stride in strides:\n",
        "\n",
        "     #set appropriate eps based on elbow crit.\n",
        "     if stride == 16 : eps = 0.15\n",
        "     elif stride == 32 : eps = 0.26\n",
        "     elif stride == 64 : eps = 0.3\n",
        "     elif stride == 128 : eps = 0.4\n",
        "\n",
        "     silhouette_scores = []\n",
        "     bad_clusters = 0  # count subsets with zero clusters\n",
        "     all_zero_clusters = 0\n",
        "     subset_num = 0  #to accurately store labels for ARI\n",
        "\n",
        "     for i in range(0, n_features, stride):\n",
        "\n",
        "      subset_length = min(stride, n_features-i)\n",
        "      subset_data = scaled_data[:, i:i+subset_length]\n",
        "      clustering = DBSCAN(eps=eps, min_samples=5).fit(subset_data)\n",
        "      # cluster results\n",
        "      labels = clustering.labels_\n",
        "      all_labels.append((stride, subset_num, labels))  # append the labels for this stride to all_labels\n",
        "      n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "      if len(set(labels)) > 1:\n",
        "        score = silhouette_score(subset_data , clustering.labels_)\n",
        "        silhouette_scores.append(score)\n",
        "      else:\n",
        "        bad_clusters += 1\n",
        "      # Count number of zero clusters\n",
        "      if  np.all(labels == 0):\n",
        "             all_zero_clusters += 1\n",
        "      subset_num = subset_num + 1\n",
        "\n",
        "     #store plots\n",
        "     methd = 'RAW'\n",
        "     comprsn = 0\n",
        "     fig = plot_confidence_interval(silhouette_scores,methd,stride,comprsn,'SC') #plot function call\n",
        "     pdf.savefig(fig)\n",
        "     plt.close(fig)\n",
        "\n",
        "     # check an exo toulaxiston ena score - avoid errors\n",
        "     if silhouette_scores:\n",
        "       avg_silhouette = np.mean(silhouette_scores)\n",
        "     else:\n",
        "       avg_silhouette = None\n",
        "     # create a file name based on the method, subset index, stride, and compression\n",
        "     avg_silhouette = np.mean(silhouette_scores)\n",
        "     file_name = f\"RAW DATA_stride({stride}).txt\"\n",
        "     # write the silhouette coefficients to file\n",
        "     with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "      f.write(f\"Number of subsets where clustering was not possible: {bad_clusters}\\n\")\n",
        "      f.write(f\"Number of clusters filled with only zeros: {all_zero_clusters}\\n\")\n",
        "      f.write(f\"Average Silhouette Score: {avg_silhouette}\\n\")\n",
        "      content = str(silhouette_scores)\n",
        "      f.write(content)\n",
        "      f.close()\n",
        "\n",
        "for stride in strides:\n",
        "    #set appropriate eps based on elbow crit.\n",
        "    if stride == 16 : eps = 0.15\n",
        "    elif stride == 32 : eps = 0.26\n",
        "    elif stride == 64 : eps = 0.3\n",
        "    elif stride == 128 : eps = 0.4\n",
        "\n",
        "    #file for storing\n",
        "    stride_folder_name = f\"stride({stride}),eps = {eps})\"\n",
        "    if not os.path.exists(os.path.join(folder_name,stride_folder_name)):\n",
        "        os.makedirs(os.path.join(folder_name,stride_folder_name))\n",
        "\n",
        "    for compression in compressions:\n",
        "\n",
        "        #files for storing measurements\n",
        "        compression_folder_name = f\"compression({compression})\"\n",
        "        if not os.path.exists(os.path.join(folder_name,stride_folder_name, compression_folder_name)):\n",
        "            os.makedirs(os.path.join(folder_name,stride_folder_name, compression_folder_name))\n",
        "\n",
        "        #files for storing plots\n",
        "        plots_dir = os.path.join(folder_name,stride_folder_name, compression_folder_name, 'plots')\n",
        "        os.makedirs(plots_dir, exist_ok=True)\n",
        "        pdf_file = os.path.join(plots_dir, 'plots.pdf')\n",
        "\n",
        "        with PdfPages(pdf_file, 'a') as pdf:\n",
        "         for method in methods:\n",
        "\n",
        "            slices = stride // compression\n",
        "            silhouette_scores = []\n",
        "            ARI_scores = []\n",
        "            bad_clusters = 0  # count subsets with zero clusters\n",
        "            all_zero_clusters = 0\n",
        "            num_subset = 0\n",
        "\n",
        "            # Iterate over subsets\n",
        "            for i in range(0, n_features, stride):\n",
        "              subset_length = min(stride, n_features-i)\n",
        "              subset_data = scaled_data[:, i:i+subset_length]\n",
        "\n",
        "              if method == 'DFT':\n",
        "                      subset_data = subset_data.T\n",
        "                      subset_transform = np.abs(fft(subset_data, axis=0))\n",
        "                      subset_transform = np.sort(subset_transform, axis=0)[::-1][:8]\n",
        "                      subset_transform = subset_transform.T\n",
        "              elif method == 'DCT':\n",
        "                     subset_data = subset_data.T\n",
        "                     subset_transform = dct(subset_data, axis=0)\n",
        "                     subset_transform = np.sort(subset_transform, axis=0)[::-1][:slices]\n",
        "                     subset_transform = subset_transform.T\n",
        "              elif method == 'DWT':\n",
        "                     subset_data = subset_data.T\n",
        "                     cA, cD = pywt.dwt(subset_data, 'db1', axis=0)\n",
        "                     subset_transform = np.concatenate((cA, cD), axis=0)\n",
        "                     subset_transform = np.sort(subset_transform, axis=0)[::-1][:slices]\n",
        "                     subset_transform = subset_transform.T\n",
        "              elif method == 'PAA':\n",
        "                     subset_transform = paa(subset_data.T, n_pieces=slices)\n",
        "                     subset_transform = subset_transform.T\n",
        "\n",
        "              clustering = DBSCAN(eps=eps, min_samples=5).fit(subset_transform)\n",
        "              labels = clustering.labels_\n",
        "\n",
        "              # cluster results\n",
        "              if len(set(labels)) > 1:\n",
        "                  score = silhouette_score(subset_transform , clustering.labels_)\n",
        "                  silhouette_scores.append(score)\n",
        "              else:\n",
        "                bad_clusters += 1\n",
        "              original_labels = [labels for strde, subst_num, labels in all_labels if strde == stride and subst_num == num_subset][0]\n",
        "              ARI = adjusted_rand_score(original_labels, labels)\n",
        "              ARI_scores.append(ARI)\n",
        "              num_subset += 1\n",
        "              # Count number of zero clusters\n",
        "              if  np.all(labels == 0):\n",
        "                       all_zero_clusters += 1\n",
        "                       silhouette_scores.append(0)\n",
        "            #store plots\n",
        "            fig2 = plot_confidence_interval(silhouette_scores,method,stride,compression,'SC') #plot function call\n",
        "            pdf.savefig(fig2)\n",
        "            plt.close(fig2)\n",
        "            fig = plot_confidence_interval(ARI_scores,method,stride,compression,'ARI') #plot function call\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "            # check an exo toulaxiston ena score - avoid errors\n",
        "            if silhouette_scores:\n",
        "             avg_silhouette = np.mean(silhouette_scores)\n",
        "            else:\n",
        "             avg_silhouette = None\n",
        "\n",
        "            # create a file name based on the method, subset index, stride, and compression\n",
        "            file_name = f\"{method}_stride({stride})_compression({compression} - SILHOUETTES).txt\"\n",
        "            # write the silhouette coefficients to file\n",
        "            with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Number of subsets where clustering was not possible: {bad_clusters}\\n\")\n",
        "              f.write(f\"Number of clusters filled with only zeros: {all_zero_clusters}\\n\")\n",
        "              f.write(f\"Average Silhouette Score: {avg_silhouette}\\n\")\n",
        "              content = str(silhouette_scores)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "            avg_ARI = np.mean(ARI_scores)\n",
        "            file_name = f\"{method}_stride({stride})_compression({compression} - ARIs).txt\"\n",
        "            # write the silhouette coefficients to file\n",
        "            with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Average ARI Score: {avg_ARI}\\n\")\n",
        "              content = str(ARI_scores)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FmyUXXell6VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Elbow criterion for eps\n",
        "\n",
        "i=0\n",
        "print(\"Original shape:\", scaled_data.shape)\n",
        "scaled_data = scaled_data[:, i:i+128]\n",
        "print(\"New shape:\", scaled_data.shape)\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import DBSCAN\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors=2)\n",
        "nbrs = neigh.fit(scaled_data)\n",
        "distances, indices = nbrs.kneighbors(scaled_data)\n",
        "\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "\n",
        "plt.plot(distances)\n",
        "#plt.xlim(40, 60)\n",
        "#plt.ylim(0, 200)\n"
      ],
      "metadata": {
        "id": "wM_US3-lBEgW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "43fa38f6-945e-4bb2-b5e8-470f8770fb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (51, 16)\n",
            "New shape: (51, 16)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79c0f7258970>]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAikAAAGhCAYAAACkmCQ2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7UklEQVR4nO3de3iUd53//9ccMjlMcucASTgTQiUCPbBUSlMq24O1YruyFmrjfrW0dGvqL7UW9LqsfhXLl6osHrf0fFooartq16oVsGxVULCsPVvWAiVAOCYhCTOTwyQzc9+/P8IMTQMhk8PMPZPn47q4ktyZ+573vDtNXrk/n899OyzLsgQAAGAzzmQXAAAAcCaEFAAAYEuEFAAAYEuEFAAAYEuEFAAAYEuEFAAAYEuEFAAAYEuEFAAAYEvuZBcwGJZlyTSH51p0Tqdj2I6N0+hzYtDnxKHXiUGfE2M4+ux0OuRwOPr12JQOKaZpqbm5bciP63Y7VVjold/frnDYHPLjoxt9Tgz6nDj0OjHoc2IMV5+LirxyufoXUhjuAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAvViWJcuykloDIQUAAPTQGYroa4++rDUbXklqHe6kPjsAALCdg8cDOnKiTWGTMykAAMBGjja1SZImlOQmtQ5CCgAA6OHoie6QMrE0L6l1EFIAAEAPx5raJRFSAACAzcTOpJQQUgAAgE10dIbVEuiUJE0sZU4KAACwiePN3UM9+bke5eZ4kloLIQUAAMREh3rGjfImuRJCCgAAeI/o8uPxxYQUAABgI8dOdA/3jBtNSAEAADYSPZNih+GeuC+Lf/DgQT355JN68803tXfvXpWXl+uFF17oc5+GhgatW7dO27dvV11dnfLy8jRnzhwtX75c48ePH3DxAABg6ITCETWe7JAkjbPBcE/cIWXv3r3aunWrLrroIpmm2a87JO7atUtbtmzRokWLdNFFF6mlpUUPP/ywbrzxRr3wwgsqKioaUPEAAGDo1Dd3yLKknEy38r3JXdkjDSCkXHXVVfrIRz4iSbrnnnv09ttvn3Ofiy++WJs2bZLbffrpZs+erSuuuELPP/+8li5dGm8ZAABgiEWHesaOzpHD4UhyNQMIKU5n/NNYDMPotW3MmDEqKipSQ0ND3McDAABDz07Lj6UBhJShsn//fjU1NWnq1KmDOo7bPfRzf10uZ4+PGB70OTHoc+LQ68Sgz8PneEv3fJQJJbm26HNSQoplWbrvvvtUUlKi6667bsDHcTodKiwcvrRnGNnDdmycRp8Tgz4nDr1ODPo89OpPXW12WtmoWH+T2eekhJS1a9fq5Zdf1hNPPKGcnJwBH8c0Lfn97UNYWTeXyynDyJbf36FIxBzy46MbfU4M+pw49Dox6PPwiJimjjS2SpKMLJf8/o5h6bNhZPf77EzCQ8rPfvYzPfjgg/rWt76lysrKQR8vHB6+N2gkYg7r8dGNPicGfU4cep0Y9Hlo1Te3Kxyx5MlwKt/riQWTZPY5oQNNW7Zs0b333qu77rpLixcvTuRTAwCAPkQnzY4pypHTBit7pASGlJ07d2r58uW68cYbVVNTk6inBQAA/RC70qwNLocfFfdwT0dHh7Zu3SpJOnLkiFpbW7V582ZJ0iWXXKKioiItWbJER48e1ZYtWyRJ+/btU01NjcrKyrRw4UK98cYbseMVFRVp0qRJQ/BSAADAQB1r6p7jOdYmy4+lAYSUpqYmffGLX+yxLfr1008/rblz58o0TUUikdj333zzTQUCAQUCAX3605/use8nP/lJrV69eiC1AwCAIXL6GikDX9Ay1OIOKRMmTNDu3bv7fMyGDRt6fH3DDTfohhtuiPepAABAAliWpWPN9rn7cRRXwgEAYIRrCXSqsysil9Oh4gL7XH+GkAIAwAgXHeopKcyW20ZX8rVPJQAAICmONtlvqEcipAAAMOIdi9792EYreyRCCgAAI54dV/ZIhBQAAEa8Ywz3AAAAu/G3d6m1IySHui+JbyeEFAAARrBjp4Z6RuVnyZPhSnI1PRFSAAAYwey6skcipAAAMKIdi02aJaQAAAAbid79eOxoe81HkQgpAACMaLGVPZxJAQAAdtEeDKsl0CnJfhdykwgpAACMWMeau4d6CnI9yslyJ7ma3ggpAACMUMdOdA/12PEsikRIAQBgxIpOmrXjfBSJkAIAwIgVW35sw5U9EiEFAIARK7qyh+EeAABgG12hiBpPdkiSxtrwarMSIQUAgBHpeHO7LEneLLeMnIxkl3NGhBQAAEag2FDPaK8cDkeSqzkzQgoAACPQ0dg9e+w5aVYipAAAMCIds/nyY4mQAgDAiPTe4R67IqQAADDCRExTx5ujy48Z7gEAADbR0NKhiGkpM8OlIiMr2eWcFSEFAIARJjrUM2ZUjpw2XdkjEVIAABhxUmFlj0RIAQBgxImt7LHxpFmJkAIAwIhz1Ob37IkipAAAMIKYlhU7k2LnlT0SIQUAgBGl2R9UV8iUy+lQSWF2ssvpEyEFAIARpKGl+87HJYXZcjntHQPsXR0AABhSDSe7Q0pxgb3PokiEFAAARpTGFkIKAACwoeiZlBJCCgAAsJPG6HCPzSfNSoQUAABGDMuyYiGFMykAAMA2WjtC6uiMSJJG59v3xoJRhBQAAEaI6HyUwrxMeTJcSa7m3OIOKQcPHtSKFSu0cOFCzZgxQ9dff32/9rMsS4899piuuOIKXXjhhbrpppv0xhtvxPv0AABggFJpZY80gJCyd+9ebd26VZMnT9bUqVP7vd/jjz+u+++/X7fccoseffRRFRcXa+nSpTp06FC8JQAAgAFIpfko0gBCylVXXaWtW7fq/vvv18yZM/u1T2dnpx599FEtXbpUt9xyiyorK/WDH/xABQUFevLJJ+MuGgAAxK8hhVb2SAMIKc4BXEL3tddeU2trqxYsWBDb5vF4dM0112jbtm1xHw8AAMTv9HCP/SfNSpI7EU9SW1srSSovL++xferUqVq/fr2CwaCysgbWMLd76Of+ulzOHh8xPOhzYtDnxKHXiUGfB67RF5QkjR3lPefvTzv0OSEhxe/3y+PxKDMzs8d2wzBkWZZ8Pt+AQorT6VBhoXeoyuzFMFLjdFiqo8+JQZ8Th14nBn2OT2coopZApyRp2pTRMryefu2XzD4nJKQMF9O05Pe3D/lxXS6nDCNbfn+HIhFzyI+PbvQ5Mehz4tDrxKDPA3OksVWSlJPpVrizSy1doT4fP1x9Nozsfp+dSUhIMQxDXV1d6uzs7HE2xe/3y+FwKD8/f8DHDoeH7w0aiZjDenx0o8+JQZ8Th14nBn2Oz7ET3X/UFxdkKxKxJFn92i+ZfU7IQFN0Lsr+/ft7bK+trdW4ceMGPB8FAAD0T2xlT4pMmpUSFFJmz56t3Nxcbdq0KbYtFArpxRdf1Pz58xNRAgAAI1psZU+KLD+WBjDc09HRoa1bt0qSjhw5otbWVm3evFmSdMkll6ioqEhLlizR0aNHtWXLFklSZmamqqurtXbtWhUVFWnatGl65plndPLkSd12221D+HIAAMCZNPpS60Ju0gBCSlNTk774xS/22Bb9+umnn9bcuXNlmqYikUiPx9x+++2yLEtPPfWUmpubNX36dD355JOaOHHiIMoHAAD90dAyAkLKhAkTtHv37j4fs2HDhl7bHA6HqqurVV1dHe9TAgCAQTBNSyd8qXXfHom7IAMAkPZaAp0KRyy5nA4VGUycBQAANhFd2TM6P0tOpyPJ1fQfIQUAgDTXmGI3FowipAAAkOaiISWVJs1KhBQAANJeQ0vqTZqVCCkAAKS9Bs6kAAAAO0rFq81KhBQAANJaWzCk9s6wJIZ7AACAjUTno+TnepSZ4UpyNfEhpAAAkMZiy49T7CyKREgBACCtpeI9e6IIKQAApLFUXdkjEVIAAEhrJ1L0arMSIQUAgLTWwJwUAABgN6GwqRZ/pySGewAAgI2c8HXIkpTpcSkvJyPZ5cSNkAIAQJp678oeh8OR5GriR0gBACBNperdj6MIKQAApKlUnjQrEVIAAEhbqXpjwShCCgAAaSqVL+QmEVIAAEhLpmXphC8oiTMpAADARnytXQqFTTkdDo0yMpNdzoAQUgAASEMNLe2SpFH5mXI5U/PXfWpWDQAA+pTq81EkQgoAAGmpMXZjwZwkVzJwhBQAANJQ48nuSbOcSQEAALYSvSR+cUFWkisZOEIKAABpqDHFrzYrEVIAAEg77cGwWjtCkggpAADARqJnUYycDGVnupNczcARUgAASDOnV/ak7lkUiZACAEDaSfW7H0cRUgAASDPRlT2pvPxYIqQAAJB20mFlj0RIAQAg7cTOpDAnBQAA2EU4Yqo50H21Wc6kAAAA22jyBWVZkifDqXyvJ9nlDAohBQCANPLelT0OhyPJ1QwOIQUAgDSSLit7JEIKAABpJV1W9kgDCCn79u3TrbfeqlmzZmnevHlas2aNurq6zrlfS0uLVqxYoSuuuEKzZs3S9ddfr2eeeWZARQMAgDNLp5AS1wX9fT6flixZorKyMq1du1b19fVavXq1gsGgVqxY0ee+X/ziF1VbW6vly5dr7Nix2rZtm+699165XC596lOfGtSLAAAA3aJzUlJ9+bEUZ0h59tln1dbWpgceeEAFBQWSpEgkopUrV6q6ulqlpaVn3K+xsVE7d+7Ud77zHd1www2SpMrKSv3tb3/Tb3/7W0IKAABDwLKs2JmUETcnZdu2baqsrIwFFElasGCBTNPU9u3bz7pfOByWJOXl5fXYnpubK8uy4ikBAACcha+tS10hUw6HNCo/K9nlDFpcZ1Jqa2u1aNGiHtsMw1BxcbFqa2vPut/YsWN1+eWX65FHHtGUKVM0ZswYbdu2Tdu3b9f3vve9gVV+its99HN/XS5nj48YHvQ5Mehz4tDrxKDPZ9fk776I2ygjS1mZcf2K78UOfY7rFfj9fhmG0Wt7fn6+fD5fn/uuXbtWy5Yt03XXXSdJcrlc+vrXv65rr702nhJ6cDodKiz0Dnj/czGM1D9Vlgroc2LQ58Sh14lBn3sL7DkhSZpYmjdkvx+T2efBxax+sixLX/3qV3XgwAF9//vfV3FxsXbs2KFvf/vbys/PjwWXeJmmJb+/fYir7U6NhpEtv79DkYg55MdHN/qcGPQ5ceh1YtDns9t3uEWSNMrIVEtL26CONVx9Nozsfp+diSukGIahQCDQa7vP51N+fv5Z9/vjH/+ozZs369e//rUqKiokSXPnzlVTU5NWr1494JAiSeHw8L1BIxFzWI+PbvQ5Mehz4tDrxKDPvR070f2He3FB9pD1Jpl9jmugqby8vNfck0AgoMbGRpWXl591v3fffVcul0vTpk3rsX369OlqaGhQR0dHPGUAAIAzqG/pDiljinKSXMnQiCukzJ8/Xzt27JDf749t27x5s5xOp+bNm3fW/caPH69IJKLdu3f32L5r1y6NGjVK2dmMKwIAMBimZam+ufuP/tKRGFKqqqrk9XpVU1OjP//5z3ruuee0Zs0aVVVV9bhGypIlS3TNNdfEvp4/f77GjRunu+66S7/61a/0l7/8Rd/97nf1y1/+Up/5zGeG7tUAADBCNfuDCkdMuZwOjTIyk13OkIhrTkp+fr7Wr1+vVatWqaamRl6vV4sXL9ayZct6PM40TUUikdjXubm5WrdunX74wx/qe9/7ngKBgCZMmKB77rmHkAIAwBCobzl9pVmXMz2WZ8e9umfq1Klat25dn4/ZsGFDr22TJ0/Wj370o3ifDgAA9EN9c/d8lNLC9BjqkbgLMgAAaeF4NKQUpc88T0IKAABpoKElvSbNSoQUAADSQvRMyhiGewAAgF2EI6ZOnOy+bw9nUgAAgG00nuyQaVnKzHCpINeT7HKGDCEFAIAUF11+XFqYLYfDkeRqhg4hBQCAFBdbfpxGQz0SIQUAgJRXn4bLjyVCCgAAKe/0cA9nUgAAgI3Elh8z3AMAAOyiMxRRS6BTEnNSAACAjUSvNOvNcis3OyPJ1QwtQgoAACmsPk2HeiRCCgAAKS06H6UkzSbNSoQUAABSWn1L9ExKei0/lggpAACktPrm9Lv7cRQhBQCAFJauy48lQgoAACmrLRhSa0dIklRSyHAPAACwiehQT0GuR1ked5KrGXqEFAAAUlQ6Lz+WCCkAAKSsdF5+LBFSAABIWaeXHxNSAACAjZxefpx+k2YlQgoAACnJsiwdP3UmpZThHgAAYBf+ti51dkXkcEjFBZxJAQAANhGdNDs6P0sZ7vT8dZ6erwoAgDRX33JqPkqaDvVIhBQAAFJS9Bop6XjPnihCCgAAKSid79kTRUgBACAFxYZ70nT5sURIAQAg5ZimpQbmpAAAALtp9gcVjphyuxwaZWQlu5xhQ0gBACDFRC/iVlKYI6fTkeRqhg8hBQCAFBO7HH5h+s5HkQgpAACknJGw/FgipAAAkHKOp/ndj6MIKQAApJjYmRSGewAAgF2EI6ZO+IKSGO4BAAA20niyQ5YlZXpcyvd6kl3OsCKkAACQQmKXwy/MkcORvsuPpQGElH379unWW2/VrFmzNG/ePK1Zs0ZdXV392re+vl5f+cpXdOmll+rCCy/UggUL9Otf/zruogEAGKliy4/T+HL4Ue54Huzz+bRkyRKVlZVp7dq1qq+v1+rVqxUMBrVixYo+921oaNBNN92kKVOmaNWqVcrNzdXevXv7HXAAAIBU3xKdNJve81GkOEPKs88+q7a2Nj3wwAMqKCiQJEUiEa1cuVLV1dUqLS09677f/e53NWbMGD3xxBNyuVySpMrKyoFXDgDACFQ/Au5+HBXXcM+2bdtUWVkZCyiStGDBApmmqe3bt591v9bWVm3atEn/8i//EgsoAAAgftG7H5cw3NNTbW2tFi1a1GObYRgqLi5WbW3tWffbtWuXQqGQ3G63PvOZz+j1119XQUGB/vmf/1l33323MjIyBla9JLd76Of+ulzOHh8xPOhzYtDnxKHXiTGS+xzsCqsl0ClJGl+cOyy/A6Ps0Oe4Qorf75dhGL225+fny+fznXW/EydOSJK+/vWv61Of+pTuvPNOvfXWW7r//vvldDr1pS99Kc6yuzmdDhUWege0b38YRvqnVDugz4lBnxOHXifGSOxz7ZHu37WG16OJ4woS8pzJ7HNcIWWgTNOUJF122WW65557JEmXXnqp2tra9NRTT6mmpkZZWfHfato0Lfn97UNaq9SdGg0jW35/hyIRc8iPj270OTHoc+LQ68QYyX3ee6BJklRSmK2WlrZhfa7h6rNhZPf77ExcIcUwDAUCgV7bfT6f8vPz+9xP6g4m71VZWalHHnlEBw8eVEVFRTylxITDw/cGjUTMYT0+utHnxKDPiUOvE2Mk9vnIie5gUlqQnbDXnsw+xzXQVF5e3mvuSSAQUGNjo8rLy8+633nnndfncTs7O+MpAwCAEWmk3P04Kq6QMn/+fO3YsUN+vz+2bfPmzXI6nZo3b95Z9xs/frymTZumHTt29Ni+Y8cOZWVlnTPEAACA91wjhZDSW1VVlbxer2pqavTnP/9Zzz33nNasWaOqqqoe10hZsmSJrrnmmh77Llu2TL///e/1rW99S9u3b9cjjzyip556SrfccotyckZGswEAGIzY1WbT/O7HUXHNScnPz9f69eu1atUq1dTUyOv1avHixVq2bFmPx5mmqUgk0mPbVVddpR/84Ad66KGH9Mwzz6ikpERf+MIX9LnPfW7wrwIAgDRgWpbag2G1BUNq7QiprSOsto6QWoMhBdq71NoRkjQyrjYrSQ7LsqxkFzFQkYip5uahn93sdjtVWOhVS0vbiJuUlUj0OTHoc+LQ68RIxz63BUP6t5+8piONbTrXL+WSgmytvmP4r9g+XH0uKvIOz+oeAAAw9P76ToMON57+ozvT41JuVoa82W55szKUm50hb3aGcrPdunhaSRIrTSxCCgAASfbq7kZJ0sLLp+i6yslyj8Cr6Z4JXQAAIInagiG9c7BFkjR3RikB5T3oBAAASfTmuycUMS2NL/aOiDsbx4OQAgBAEkWHei6eVpzkSuyHkAIAQJIEu8J6e3+zJOniipEzIba/CCkAACTJ32qbFQqbKinI1oRib7LLsR1CCgAASfLq7gZJ0sUVxXI4HEmuxn4IKQAAJEEoHNGb+5okSbMrmI9yJoQUAACSYNeBFnV2RVSYl6kpY41kl2NLhBQAAJLgtVOremZPK5aToZ4zIqQAAJBg4Yip1/ey9PhcCCkAACTYnkMn1RYMKy8nQ9MmFiS7HNsipAAAkGCv7uk+i/IPHxgtp5OhnrMhpAAAkECmZem1PdH5KFzArS+EFAAAEqj2iF++1i5lZ7o0o6ww2eXYGiEFAIAEenVP9wXcLjpvNHc8Pge6AwBAgliW9Z4bCjLUcy6EFAAAEqSuvlUnfEF53E6dX16U7HJsj5ACAECCRId6LigfpcwMV5KrsT9CCgAACRIb6uFePf1CSAEAIAGOnmjTsaZ2uZwOXTh1dLLLSQmEFAAAEiB6AbcZZUXKyXInuZrUQEgBACABXmOoJ26EFAAAhlnjyQ4drA/I4ZBmfYChnv4ipAAAMMyil8GvmFggI8eT5GpSByEFAIBhFp2PcnEFF3CLBzN3AAAYoD2HTurHL+5WS6Czz8e1BcOSpNnTmI8SD0IKAABxMi1LG/9yUL/8U60sq3/7nF9epMK8zOEtLM0QUgAAiIO/rUuPv/C/2rW/WZJUOXOMPl45WU5H3/uVFGYnoLr0QkgBAKCfdte16NFf79LJ1i553E79n49O0+UXjJXDcY6EggEhpAAAcA6mZem3fzmo508N74wdlaP/75/P1/ji3GSXltYIKQAA9OH9wzuXnT9Gn/1ohTI93CBwuBFSAAA4i3cOtujR3+yS79Twzmc+WqHLLxyb7LJGDEIKAADv4Wvr0v/8vV4v76rX/mN+SQzvJAshBQAw4gW7wnp97wn9Zddx/e/+Fpmn1hU7HQ5dfuEYffrqaQzvJAEhBQAwIkVMU7v2t+jlXcf12t5GdYXM2PfKxxm6dEapLpleKsPLZeyThZACABhxmv1BfWvDqz2uFFtSmK3KmWN06YxSlRblJLE6RBFSAAAjzmt7GtUS6FROpluV54/RpTNLVT7W4HonNkNIAQCMOLsPnZQkXTt3kv7psrKk1oKzi/suyPv27dOtt96qWbNmad68eVqzZo26urriOsa6detUUVGh6urqeJ8eAIBBsSxLe06FlIqJBUmtBX2L60yKz+fTkiVLVFZWprVr16q+vl6rV69WMBjUihUr+nWMxsZGPfjggxo1atSACgYAYDCONrUr0B5ShtupKWONZJeDPsQVUp599lm1tbXpgQceUEFBgSQpEolo5cqVqq6uVmlp6TmP8d3vfldXXXWVjh49OqCCAQAYjD11LZKkqeMMZbjjHlBAAsX1X2fbtm2qrKyMBRRJWrBggUzT1Pbt28+5/yuvvKL//u//1pe+9KW4CwUAYChE56NUTCpMbiE4p7hCSm1trcrLy3tsMwxDxcXFqq2t7XPfSCSiVatW6Y477lBJSUn8lQIAMEiWZWl33UlJzEdJBXEN9/j9fhlG7/G7/Px8+Xy+Pvf96U9/qo6ODt1yyy1xFXgu7mE4VedyOXt8xPCgz4lBnxOHXifGYPp8rKlNvrYuuV0OTZtUMCy/Q9KFHd7PCVmC3NTUpPvvv1//9m//Jo9n6K7c53Q6VFjoHbLjvZ9hZA/bsXEafU4M+pw49DoxBtLn/9ndKEmqmFyk0hImzfZHMt/PcYUUwzAUCAR6bff5fMrPzz/rfv/+7/+uiooKfehDH5Lf332zpnA4rHA4LL/fr5ycHLnd8ecl07Tk97fHvd+5uFxOGUa2/P4ORSLmuXfAgNDnxKDPiUOvE2MwfX7t7/WSuifNtrS0DUd5aWO43s+Gkd3vszNxJYPy8vJec08CgYAaGxt7zVV5r/379+uvf/2r5syZ0+t7c+bM0eOPP6758+fHU0pMODx8PwgiEXNYj49u9Dkx6HPi0OvEiLfPlmXp7we7V/Z8YEI+/436KZnv57hCyvz58/XII4/0mJuyefNmOZ1OzZs376z7fe1rX4udQYn69re/raysLC1fvlwVFRUDKB0AgP5r9AXVEuiUy+nQeePOfvYf9hFXSKmqqtKGDRtUU1Oj6upq1dfXa82aNaqqqupxjZQlS5bo6NGj2rJliyRp+vTpvY5lGIZycnI0d+7cQb4EAADObc+pVT1lY/OU6XEltxj0S1xTdvPz87V+/Xq5XC7V1NTo+9//vhYvXqx77rmnx+NM01QkEhnSQgEAGIzdh7qHeiomcn2UVBH3bNWpU6dq3bp1fT5mw4YN5zxOfx4DAMBQiV0fZVJBUutA/7FAHACQ9pr9QZ3wBeV0OHTeeOajpApCCgAg7UXPokwek6vszIRcIgxDgJACAEh7zEdJTYQUAEDai55JmcZ8lJRCSAEApLWTrZ2qb+mQQ9K0CcxHSSWEFABAWoueRZlYmqucrIzkFoO4EFIAAGlt96GTkpiPkooIKQCAtLa77tSkWeajpBxCCgAgbfnbunSsqV2SNG1iQXKLQdwIKQCAtLXn1FDPhGKvcrOZj5JqCCkAgLQVuxQ+81FSEiEFAJC2YhdxYz5KSiKkAADSUmtHSIcb2yQxHyVVEVIAAGkpOh9l7KgcGV5PcovBgBBSAABpKTYfZRLzUVIVIQUAkJZO31SwILmFYMAIKQCAtNMeDOlQfaskJs2mMkIKACDt7DnskyWptDBbBbmZyS4HA0RIAQCknT2x+SgFSa0Dg0NIAQCkndPzUZg0m8oIKQCAtNLsD+rgceajpAN3sgsAAGAgLMtSw8kO1R0PqK4hoLr6VtXVB3SytUuSNDo/S0VGVpKrxGAQUgAAKcE0Lb17xKc39zXpUGOrag/71N4Z7vU4h6SSohwtnFeW8BoxtAgpAADbipim9tSd1Ct7GvXa7kb52rp6fN/ldGh8sVeTSvM0uTRPk0pzNbEkV1kefr2lA/4rAgBsJRwx9c7BFr2yu1Gv7WlUa0co9r3sTLdmTxutD80Yq2LDo5KCbLldTK9MV4QUAIAtHG5s1Yt/PaTX9zSqLXh6GMeb5dbsacW6uKJEM8oKlZXpVmGhVy0tbQqHzSRWjOFGSAEAJFVdfUC/2XFAr+5ujG0zcjI0u6JEF1cUq2JiAWdLRihCCgAgKfYf8+s32w/ojXdPxLZ9qKJYV188QR+YUCCn05HE6mAHhBQAQELtO+LTb3Yc0Fv7miR1r8aZM71E119WpgnFucktDrZCSAEADFqzPyh/e1efj2ltD+l3fz2kXfubJUkOh3TpjDG6/rLJGjvKm4gykWIIKQCAQfnj60e04cXdsqz+Pd7pcOiy88foussmq7QwZ3iLQ0ojpAAABux/DzTrxy/ukWVJ+bkeufqYR+J0ODRzSpE+fulkFRdkJ7BKpCpCCgBgQI43t+vh59+WaVmqnFmqf71+hhwOJrti6LCmCwAQt7ZgSP/+i7fUFgxr6jhDtyz4IAEFQ46QAgCIS8Q09fDzb6u+uV1FRqbuXHShMtyuZJeFNERIAQDE5dn/flf/e6BFngyn7lp0ofK9nmSXhDRFSAEA9NsfXj+il147LEm6/fqZmlSal+SKkM4IKQCAfvn7gWb95MU9kqQb5pfr4oriJFeEdEdIAQCcU31zux46tZLn0pmluq5ycrJLwghASAEA9Kn9fSt5bmUlDxKE66QAwAgVCpv6W22TOjrDfT5ux9vHdTy6kueGC1jJg4SJO6Ts27dP9913n15//XV5vV4tXLhQd999tzyes8/ubmho0Lp167R9+3bV1dUpLy9Pc+bM0fLlyzV+/PhBvQAAQPze2ndCz/z3XtW3dPTr8bGVPLmZw1wZcFpcIcXn82nJkiUqKyvT2rVrVV9fr9WrVysYDGrFihVn3W/Xrl3asmWLFi1apIsuukgtLS16+OGHdeONN+qFF15QUVHRoF8IAODc6pvb9cxLe2N3IDa8Hk0q7fvOwxkupz46ZyIreZBwcYWUZ599Vm1tbXrggQdUUFAgSYpEIlq5cqWqq6tVWlp6xv0uvvhibdq0SW736aebPXu2rrjiCj3//PNaunTpwF8BAOCcgl1h/WbHAb34P4cUMS25nA5dM2ei/umyMmVnMvIPe4pr4uy2bdtUWVkZCyiStGDBApmmqe3bt591P8MwegQUSRozZoyKiorU0NAQX8UAgH6zLEt/2XVcX3vsZW16uU4R09L5U4r0/267RJ+68jwCCmwtrndnbW2tFi1a1GObYRgqLi5WbW1tXE+8f/9+NTU1aerUqXHt935u99AvUHK5nD0+YnjQ58Sgz4mTyF53dIYVjph9PqahpUM/3bJHew/7JEklBdn6Px+dplkfGJ3Sq3N4TyeGHfocV0jx+/0yDKPX9vz8fPl8vn4fx7Is3XfffSopKdF1110XTwk9OJ0OFRZ6B7z/uRgGtxJPBPqcGPQ5cYa6120dIb17+KT2HjqpvYdatPfQSTX2c8KrJGV6XLrpI9O0cP5UeTLSZ2UO7+nESGafk3Keb+3atXr55Zf1xBNPKCcnZ8DHMU1Lfn/7EFbWzeVyyjCy5fd3KHKOv1QwcPQ5Mehz4pyr15Zl6UhjmzpDkT6PEwqbqqsPqPaoX/uP+XWsaWA/55wOhy6ZUaKqqz+gIiNLba1BtQ3oSPbCezoxhqvPhpHd77MzcYUUwzAUCAR6bff5fMrPz+/XMX72s5/pwQcf1Le+9S1VVlbG8/RnFA4P3xs0EjGH9fjoRp8Tgz4nzpl6/c7BFv38j/u0/5h/QMccZWRpytg8TRlrqGysocmlecrKPPdZEeepYZ10/G/PezoxktnnuEJKeXl5r7kngUBAjY2NKi8vP+f+W7Zs0b333qu77rpLixcvjq9SAEhBdfUB/WLrPr1d2yxJ8ridMs5x12CHQxo7yqspYw1NGZunsjHGOfcB0lFcIWX+/Pl65JFHesxN2bx5s5xOp+bNm9fnvjt37tTy5ct14403qqamZuAVA0AKaDzZoV/+qVY7d9XLkuRyOvSPs8bpn+ZNUT6BA+iXuEJKVVWVNmzYoJqaGlVXV6u+vl5r1qxRVVVVj2ukLFmyREePHtWWLVskdV+ltqamRmVlZVq4cKHeeOON2GOLioo0adKkoXk1AJBkvtZO/fh3u/XSq4cVMS1J0iXTS/TJ+eUqLRz4HDxgJIorpOTn52v9+vVatWqVampq5PV6tXjxYi1btqzH40zTVCRyemLYm2++qUAgoEAgoE9/+tM9HvvJT35Sq1evHsRLAIDh0xWK6GhTm443tcdCx9k0+oLa8tc6dXR2//ybWVaoxVecp8ljuFIrMBAOy7L6/r/OxiIRU83NQz9X3e12qrDQq5aWNiZlDSP6nBj0uX8sy1JLoFOHG1t1qOH0v+PN7Yr3p2TZ2Dwt+sepmlnGLT+GA+/pxBiuPhcVeYdndQ8ApItQ2FTtUZ/+frBFew/7VFcfUFvwzHcDzs3O0LjRXnnOcfHIjAynrrmkTDMm58uMpOzff4BtEFIAjAimaelgfUB/P9jSHUwOnVTX+/46dDocGjsqRxNKcjWxJFcTirs/FuR6+nWF1vf+5WmKkAIMFiEFQFrqCkV0qLFVtUf9eudgi96pO6mOzp5nSoycDH1wcqE+OLlQU8YYGjc6Rxnu9LkiK5DqCCkAUl5HZ1iHGlp18HhAB+u7/x070S7zfZNJsjPd+uCkAn1wcqFmTC7UuNHelL6HDZDuCCkAUk5DS7t2HWjR7roWHaxvVX3zmS8bb+RkaPIYQxWTCjR9cqEml+bJ6SSUAKmCkALA9lo7QnrnYIt2HWjWrv3NOuEL9npMkZGpyaV5mlyap0ljuj/2dy4JAHsipACwHcuytOfQSb29v1n/e6BZB44HeiwDdjkdmjrO0IyyIpWPNzSpNE9GDldxBdINIQWA7fzij/u0aWddj21jR+VoZlmRZkwpUsXEAmVn8uMLSHf8Xw7AVvYf82vzqYByyfQSnT9llGaUFarIyEpyZQASjZACwDYipqn1m96RJalyZqlu/6eZyS4JQBL177q0AJAAW/56WHUNrfJmuXXTVR9IdjkAkoyQAsAWTpzs0PN/rpUkferK82R4mQgLjHSEFABJZ1mWfrxlj7pCpqZNLNDlF45NdkkAbICQAiDpXtndqLf2NcnldGjJxyq4tgkASYQUAEnWHgzrp1v2SJKuq5yssaO8Sa4IgF0QUgAk1XNb98nX1qXSohxdVzk52eUAsBFCCoCkefeIT398/Ygk6eZrK7gDMYAeCCkAkiIcMbV+c/c1UeZdMEbTJxcmuyQANkNIAZAUv/ufOh1pbFNudoY+deV5yS4HgA0RUgAkXMPJDv16+wFJ0k1Xnac8bg4I4AwIKQASyrIsbfjdboXCpqZPLtRl549JdkkAbIp79wAYFqFwRM2BTjX7gt0f/UE1+TvV0NKud+pOyu1y6rPXck0UAGdHSAHQLy/vOq4/vnFUkYjZ5+NCEVMtgU4F2kN9Pm7h5WUaU5QzlCUCSDOEFAB9CoVNPfPS3thS4Xh4MpwqystSkZGpIiNLRXmZGmVkadxor6aOzx+GagGkE0IKgLM64evQQ798WweOB+SQ9PHKySofZ/S5j9PhUGFedyjxZrkZzgEwYIQUAGf0t9omPfbrXWoLhuXNcqv6EzN1fvmoZJcFYAQhpADowbQs/Wb7Af36z/tlSZoyNk+f/+fzNTo/O9mlARhhCCkAYlo7QnrsN7v0dm2zJOnKfxivqqs/oAw3VysAkHiEFACSpP3H/Hrol39Tk79THrdTN3+sQpedPzbZZQEYwQgpwAgXCpt68a91+tWf9yscsVRSmK07P3mBJpTkJrs0ACMcIQUYoSzL0ut7T+g/f79XjSeDkqTZ04q19OPTlZPFjwYAycdPImAEOtzQqmde2qu/H2yRJOXnerT4H6fqsvPHsGQYgG0QUoARJNDepef/tF9/fOOILEtyu5y69pKJuq5ysrI8/DgAYC/8VAJGgHDE1OaddXp+W63aO8OSpA9VFOvGK89TcQFLiwHYEyEFSCOWZam9M6xmf6ea/EG1+INqae3S63tP6EhjqyRpYkmu/uUjH1DFpMIkVwsAfSOkAEliWpY6uyIKdkUU7AqrMxRRsPP018GuiLpCEUUsS6ZpKWKe+WNnKBK7y3Czv1OdocgZny8vJ0M3zC/Xhy8cJ6eTeScA7I+QghEr+gs+2BVRZyiizq6IusIRmdEQcJZwEAqbaguG1RYMdX/sCKk9GFJr7PPugHHO57esYXtteTkZsRv7jS7I1pTxBZp93ih5uCgbgBRCSEFKC0dMHWlsU+PJDnV0hdXRGVGwM3zq8+6vO059HeyKxM5cdIYiCoXNZJcvSXI4pCyPS1ke96mPpz/PzHDJ5XTI6XTEPr73c5fToQyXU4WnAskoI0uFeZnyZLhix3e7nSos9KqlpU1hm7xmAOgPQgpSRkdnWIcaWlVXH1BdfffHIyfaFDEHd0YiGhIyM1zyuF1yuU4FAMf7wsGprzPcTuVkueXNypA3+jE7+nX35/1ZKeN0OpSV4ZInw8myXwA4A0IKhoRlWQp2RdTWEVKgI9Q97NEZVjhiKhzpHiIJhU2FI6c/RixLGRludXaGZPYRNALtIdXVB9TQ0qEzPSon062xo3KUk5Wh7EyXsjPdyva4lZ3pUlamWzmZ3aEhK/PUWYqM7kCSeeqshdtFSAAAO4o7pOzbt0/33XefXn/9dXm9Xi1cuFB33323PB5Pn/tZlqXHH39cP/3pT9Xc3Kzp06frq1/9qmbNmjXQ2jEIsVAR7J5D0dYRis2z6OyKKGxasYARiZiKvO/rYOh0IGk9FUrCkeGbYxFVmJepSSW5mlSad+pfrkbnZxEyACANxRVSfD6flixZorKyMq1du1b19fVavXq1gsGgVqxY0ee+jz/+uO6//359+ctfVkVFhX7yk59o6dKl+tWvfqWJEycO6kWMRO3BsI6eaNORE606cqJNR0+0KdAeOud+3ZM+u4PJYIdJzsTtciovp3vYIyfTpYwMlzJcTrld3cMkbpdTbrdTGS6nMjNcyvF61Bns+0xKpseliaeCiZHTdxgGAKSPuELKs88+q7a2Nj3wwAMqKCiQJEUiEa1cuVLV1dUqLS09436dnZ169NFHtXTpUt1yyy2SpIsvvlgf+9jH9OSTT+ree+8dzGtIKaGweWpCZ1jtpz6GI31PZrQsyd/WFQsjR060qSXQOST1uF2O7kCR5ZY3O0O5WRnK9Ljkdjm6A4XTKVf0c5dDLpdTbqdDngyXcrMzev2LZ34FEzoBAH2JK6Rs27ZNlZWVsYAiSQsWLNA3v/lNbd++XTfccMMZ93vttdfU2tqqBQsWxLZ5PB5dc8012rJly8AqH0aW1b00tSsUOesvT0tSZyii1vaQAu1dau0IKdDePfwR/bq1PdR91uLUCpP24LkDSTwK8zI1brRX40d7NW60V0V5mdI58oHb6ZQ3+9SEz+wMedzMxwAA2FNcIaW2tlaLFi3qsc0wDBUXF6u2trbP/SSpvLy8x/apU6dq/fr1CgaDysrKiqeUGPcQX/fBsizd9/Sr2nvo5JAe9/2yPC7lZLqVneVWhtspxznSRU6WW+OLvRo/OlcTir0aV+yVNytjWGscbi6Xs8dHDA/6nDj0OjHoc2LYoc9xhRS/3y/DMHptz8/Pl8/n63M/j8ejzMzMHtsNw5BlWfL5fAMKKU6nQ4WF3rj364tlWXFdjTM3O0OG16P83EwZXk/sX/TrvBxPbCjFm5WhnOwMZWe65eKKnzGGwb1jEoE+Jw69Tgz6nBjJ7HNKL0E2TUt+f/uQH/cbSz4kT5ZHgUCHIn0Mz3TP04gnYVoKBbsUCnYNvsg04HI5ZRjZ8vv77jMGhz4nDr1ODPqcGMPVZ8PI7vfZmbhCimEYCgQCvbb7fD7l5+f3uV9XV5c6Ozt7nE3x+/1yOBx97nsuwzHh0u12KjvTrWC7U46+FsBYw/P8I00kYtLHBKDPiUOvE4M+J0Yy+xzXQFN5eXmvuSeBQECNjY295pu8fz9J2r9/f4/ttbW1Gjdu3IDnowAAgPQVV0iZP3++duzYIb/fH9u2efNmOZ1OzZs376z7zZ49W7m5udq0aVNsWygU0osvvqj58+cPoGwAAJDu4hruqaqq0oYNG1RTU6Pq6mrV19drzZo1qqqq6nGNlCVLlujo0aOx5cWZmZmqrq7W2rVrVVRUpGnTpumZZ57RyZMnddtttw3tKwIAAGkhrpCSn5+v9evXa9WqVaqpqZHX69XixYu1bNmyHo8zTVORSM9b1d9+++2yLEtPPfVU7LL4Tz75JFebBQAAZ+SwLGv4b7gyTCIRU83NbUN+XK6Emhj0OTHoc+LQ68Sgz4kxXH0uKvL2e3UPV8IBAAC2REgBAAC2REgBAAC2REgBAAC2REgBAAC2REgBAAC2REgBAAC2REgBAAC2lNIXc7MsS6Y5POW7XE5uAZ4A9Dkx6HPi0OvEoM+JMRx9djodcjgc/XpsSocUAACQvhjuAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIAQAAtkRIeY99+/bp1ltv1axZszRv3jytWbNGXV1dyS4rpR08eFArVqzQwoULNWPGDF1//fVnfNzPf/5zXXvttbrgggv0iU98Qn/4wx8SXGlq27Rpkz7/+c9r/vz5mjVrlhYuXKhf/OIXev9Nzunz4GzdulWf+cxndOmll+r888/X1Vdfre985zsKBAI9Hvf73/9en/jEJ3TBBRfo2muv1XPPPZekitNDW1ub5s+fr4qKCv3tb3/r8T3e0wP3X//1X6qoqOj173vf+16PxyWzx+6EPZPN+Xw+LVmyRGVlZVq7dq3q6+u1evVqBYNBrVixItnlpay9e/dq69atuuiii2SaZq9fmpL029/+Vt/4xjd0xx136NJLL9XGjRt155136ic/+YlmzZqV+KJT0Lp16zR+/Hjdc889Kiws1I4dO/SNb3xDx48f15133imJPg+FkydP6sILL9RnP/tZFRQUaO/evVq7dq327t2rp556SpL0yiuv6M4779TixYv1ta99TS+//LL+7//9v/J6vfrYxz6W5FeQmh566CFFIpFe23lPD40nnnhCeXl5sa9LS0tjnye9xxYsy7KsRx55xJo1a5bV0tIS2/bss89a06dPt44fP568wlJcJBKJff6Vr3zFuu6663o95qMf/ai1fPnyHttuuukm61//9V+Hvb500dTU1Gvb17/+dWv27Nmx/wb0eXj853/+pzVt2rTYz4mlS5daN910U4/HLF++3FqwYEEyykt57777rjVr1izrmWeesaZNm2a99dZbse/xnh6c5557zpo2bdoZf35EJbvHDPecsm3bNlVWVqqgoCC2bcGCBTJNU9u3b09eYSnO6ez7LXbo0CEdOHBACxYs6LH94x//uP7yl78w3NZPRUVFvbZNnz5dra2tam9vp8/DKPozIxQKqaurSzt37ux1xuTjH/+49u3bp8OHDyehwtR23333qaqqSlOmTOmxnff08LNDjwkpp9TW1qq8vLzHNsMwVFxcrNra2iRVlf6ivX3/D6CpU6cqFArp0KFDySgrLbz66qsqLS1Vbm4ufR5ikUhEnZ2d2rVrlx588EFdddVVmjBhgurq6hQKhXr9LJk6daok8bMkTps3b9aePXtUU1PT63u8p4fO9ddfr+nTp+vqq6/Wo48+Ghtas0OPmZNyit/vl2EYvbbn5+fL5/MloaKRIdrb9/c++jW9H5hXXnlFGzdu1Fe+8hVJ9HmoXXnllaqvr5ckffjDH9b3v/99SfR5KHV0dGj16tVatmyZcnNze32fXg9ecXGxvvCFL+iiiy6Sw+HQ73//e/3oRz9SfX29VqxYYYseE1KANHP8+HEtW7ZMc+fO1c0335zsctLSY489po6ODr377rt6+OGHdccdd+g//uM/kl1WWnn44Yc1atQoLVq0KNmlpK0Pf/jD+vCHPxz7+vLLL1dmZqbWr1+vO+64I4mVncZwzymGYfRaRih1J8X8/PwkVDQyRHv7/t77/f4e30f/+P1+3X777SooKNDatWtjc4Lo89D64Ac/qH/4h3/QjTfeqIceekg7d+7Uli1b6PMQOXLkiJ566indddddCgQC8vv9am9vlyS1t7erra2NXg+TBQsWKBKJ6O9//7stekxIOaW8vLzXeHEgEFBjY2Ov8WUMnWhv39/72tpaZWRkaOLEickoKyUFg0FVV1crEAj0WlJIn4dPRUWFMjIyVFdXp0mTJikjI+OMfZbEz5J+Onz4sEKhkD73uc9pzpw5mjNnTuwv+5tvvlm33nor7+kEsEOPCSmnzJ8/Xzt27IglRKl70pbT6dS8efOSWFl6mzhxosrKyrR58+Ye2zdu3KjKykp5PJ4kVZZawuGw7r77btXW1uqJJ57ocZ0DiT4PpzfffFOhUEgTJkyQx+PR3Llz9bvf/a7HYzZu3KipU6dqwoQJSaoytUyfPl1PP/10j39f/epXJUkrV67UN7/5Td7Tw2Tjxo1yuVyaMWOGLXrMnJRTqqqqtGHDBtXU1Ki6ulr19fVas2aNqqqqev3AR/91dHRo69atkrpP4ba2tsbe8JdccomKior0hS98QV/+8pc1adIkzZ07Vxs3btRbb72lH//4x8ksPaWsXLlSf/jDH3TPPfeotbVVb7zxRux7M2bMkMfjoc9D4M4779T555+viooKZWVl6Z133tGTTz6piooKfeQjH5Ekff7zn9fNN9+se++9VwsWLNDOnTv1wgsv6Ic//GGSq08dhmFo7ty5Z/zezJkzNXPmTEniPT1It912m+bOnauKigpJ0ksvvaSf/exnuvnmm1VcXCwp+T12WNYZLgE6Qu3bt0+rVq3S66+/Lq/Xq4ULF2rZsmUk8kE4fPiwrr766jN+7+mnn479IPr5z3+uxx9/XEePHtWUKVO0fPlyXXnllYksNaVdddVVOnLkyBm/99JLL8X+gqfPg/PYY49p48aNqqurk2VZGj9+vK655hrddtttPVagvPTSS/rRj36k/fv3a9y4cfrc5z6nxYsXJ7Hy1Ldz507dfPPN+sUvfqELLrggtp339MDdd999+tOf/qTjx4/LNE2VlZXpxhtv1Gc/+1k5HI7Y45LZY0IKAACwJeakAAAAWyKkAAAAWyKkAAAAWyKkAAAAWyKkAAAAWyKkAAAAWyKkAAAAWyKkAAAAWyKkAAAAWyKkAAAAWyKkAAAAW/r/AREs471FdPj3AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}