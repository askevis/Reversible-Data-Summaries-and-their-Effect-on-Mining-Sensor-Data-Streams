{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXS2Rwdt9k76"
      },
      "outputs": [],
      "source": [
        "!pip install pyts\n",
        "!pip install --upgrade pyts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovi31xNVuAGt",
        "outputId": "5231a618-da3a-4719-f43b-b57fb4f4faf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c8585b444d62>:159: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df.iloc[:, -1] = le.fit_transform(df.iloc[:, -1])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "import sys\n",
        "from scipy import stats\n",
        "from scipy.fft import fft, dct\n",
        "import pywt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import os\n",
        "from pyts.approximation import PiecewiseAggregateApproximation as PAA\n",
        "from scipy.fftpack import idct\n",
        "\n",
        "def generate_random_vectors(n, r):\n",
        "    random_vectors = np.random.normal(0, 1, size=(r, n))\n",
        "    return random_vectors\n",
        "\n",
        "def MINE(data,R,R_inv):\n",
        "\n",
        "  prod = np.matmul(data,R)\n",
        "  bitmap = np.where(prod < 0, 0, 1)\n",
        "  recon = np.matmul(bitmap,R_inv)\n",
        "  return recon\n",
        "\n",
        "\n",
        "def plot_confidence_interval(scores,method,stride,compression,type):\n",
        "    # compute mean silhouette score and confidence interval\n",
        "    mean_score = np.mean(scores)\n",
        "    ci = stats.t.interval(0.95, len(scores)-1, loc=mean_score, scale=stats.sem(scores))\n",
        "\n",
        "    # 2D array with the lower and upper bounds of the confidence interval\n",
        "    yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "    # create the figure and axis objects\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # plot the data with error bars\n",
        "    ax.plot(scores, marker='o')\n",
        "    ax.errorbar(x=range(len(scores)), y=scores, yerr=yerr, fmt='none', ecolor='r')\n",
        "\n",
        "    # set the axis labels and title\n",
        "    ax.set_xlabel('Subset of features')\n",
        "\n",
        "    if type == 'SC' :\n",
        "     ax.set_ylabel('R2 score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'R2-scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'R2-scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "\n",
        "\n",
        "    if type == 'RMSE' :\n",
        "     ax.set_ylabel('RMSE score')\n",
        "     if method == 'RAW':\n",
        "      ax.set_title(f'RMSE-scores with 95% confidence interval\\n M={method} , W= {stride} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "     else:\n",
        "      ax.set_title(f'RMSE-scores with 95% confidence interval\\n M={method} , W= {stride}, C= {compression} : [{ci[0]:.5f}, {ci[1]:.5f}]')\n",
        "    # return the figure object\n",
        "    return fig\n",
        "\n",
        "def plot_score_confidence_interval(data,window_size,compression):\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(20, 4), sharey=True)  # Adjust the figure size as needed\n",
        "\n",
        "    colors = ['b', 'g', 'r', 'c', 'm']  # Colors for each method\n",
        "\n",
        "    #y_axis_limits = (-5, 5)  # Outlier limits\n",
        "\n",
        "    for i, (scores, method) in enumerate(data):\n",
        "        ax = axs[i]\n",
        "        n = len(scores)\n",
        "        x = np.arange(n) + 1\n",
        "        y = np.array(scores)\n",
        "\n",
        "        # Identify and count outliers based on the limits\n",
        "        outliers = np.where((y < -5) | (y > 5))[0]\n",
        "        num_outliers = len(outliers)\n",
        "\n",
        "        # Exclude outliers from the data\n",
        "        filtered_scores = np.delete(y, outliers)\n",
        "        filtered_x = np.delete(x, outliers)\n",
        "\n",
        "        mean = np.mean(filtered_scores)\n",
        "        # Calculate the confidence interval\n",
        "        ci = stats.t.interval(0.95, len(filtered_scores), loc=mean, scale=stats.sem(filtered_scores))\n",
        "        # Plot the non-outlier data points and mean\n",
        "        ax.plot(filtered_x, filtered_scores, 'o', markersize=4, color=colors[i])\n",
        "        ax.plot(x, [mean] * n, '--', color=colors[i])\n",
        "\n",
        "        fig.suptitle(f'R2-scores with 95% confidence interval // W= {window_size}, C= {compression} ')\n",
        "\n",
        "        ax.set_xlabel('Window')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title(f'{method} : [{ci[0]:.5f}, {ci[1]:.5f}] ')\n",
        "        ax.grid(True)\n",
        "\n",
        "        # Set the y-axis limits to the predefined range\n",
        "        #ax.set_ylim(y_axis_limits)\n",
        "\n",
        "        # Add the number of outliers as text in the top right corner\n",
        "        ax.text(0.85, 0.9, f'Outliers: {num_outliers}', transform=ax.transAxes, fontsize=10, color='red', ha='right')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_RMSE_confidence_interval(data, window_size, compression, outlier_threshold=3):\n",
        "    fig, axs = plt.subplots(1, 5, figsize=(20, 4), sharey=True)  # Adjust the figure size as needed\n",
        "\n",
        "    colors = ['b', 'g', 'r', 'c', 'm']  # Colors for each method\n",
        "\n",
        "   # y_axis_limits = (-5, 5) # Outlier limits\n",
        "\n",
        "    for i, (scores, method) in enumerate(data):\n",
        "        ax = axs[i]\n",
        "        n = len(scores)\n",
        "        x = np.arange(n) + 1\n",
        "        y = np.array(scores)\n",
        "        # Identify and count outliers based on the limits\n",
        "        outliers = np.where((y < -5) | (y > 5))[0]\n",
        "        num_outliers = len(outliers)\n",
        "\n",
        "        # Exclude outliers from the data\n",
        "        filtered_scores = np.delete(y, outliers)\n",
        "        filtered_x = np.delete(x, outliers)\n",
        "\n",
        "        mean = np.mean(filtered_scores)\n",
        "        # Calculate the confidence interval\n",
        "        ci = stats.t.interval(0.95, len(filtered_scores), loc=mean, scale=stats.sem(filtered_scores))\n",
        "\n",
        "        # Plot the non-outlier data points and mean\n",
        "        ax.plot(filtered_x, filtered_scores, 'o', markersize=4, color=colors[i])\n",
        "        ax.plot(x, [mean] * n, '--', color=colors[i])\n",
        "\n",
        "        fig.suptitle(f'RMSE-scores with 95% confidence interval // W= {window_size}, C= {compression} ')\n",
        "\n",
        "        ax.set_xlabel('Window')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title(f'{method} : [{0}, {ci[1]:.5f}]')\n",
        "        ax.grid(True)\n",
        "\n",
        "        # Set the y-axis limits to the predefined range\n",
        "       # ax.set_ylim(y_axis_limits)\n",
        "\n",
        "        # Add the number of outliers as text in the top right corner\n",
        "        ax.text(0.85, 0.9, f'Outliers: {num_outliers}', transform=ax.transAxes, fontsize=10, color='red', ha='right')\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/pump_sensor2.txt', delimiter=',')\n",
        "\n",
        "# BROKEN,NORMAL,RECOVERING -> 0,0.5,1\n",
        "le = LabelEncoder()\n",
        "df.iloc[:, -1] = le.fit_transform(df.iloc[:, -1])\n",
        "max_label = df.iloc[:, -1].max()\n",
        "df.iloc[:, -1] = df.iloc[:, -1] / max_label * 1.0\n",
        "# Discard the first row + first two columns\n",
        "df = df.iloc[:, 2:]\n",
        "data = df.to_numpy(dtype='float')\n",
        "# Split  features (X) and target (y)\n",
        "X = data[:, :-1]\n",
        "#print(X)\n",
        "y = data[:, -1]\n",
        "#print(y[0])\n",
        "# Fill Nan values based on the mean of each sensor\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "# Select 1000 0.5 kai 1 labels\n",
        "label_0_indices = np.where(y == 0)[0]\n",
        "label_05_indices = np.where(y == 0.5)[0]\n",
        "label_1_indices = np.where(y == 1)[0]\n",
        "label_05_indices = np.random.choice(label_05_indices, size=1000, replace=False)\n",
        "label_1_indices = np.random.choice(label_1_indices, size=1000, replace=False)\n",
        "# Concatenate + shuffle\n",
        "indices = np.concatenate((label_0_indices, label_05_indices, label_1_indices))\n",
        "np.random.seed(123)   #Gia idio dataset shuffle se ola ta algorithms\n",
        "np.random.shuffle(indices)\n",
        "X= X[indices]\n",
        "y= y[indices]\n",
        "\n",
        "#print\n",
        "# unique_labels, label_counts = np.unique(y, return_counts=True)\n",
        "# for i in range(len(unique_labels)):\n",
        "#    print(f\"Label {unique_labels[i]}: {label_counts[i]}\")\n",
        "# X = imputer.fit_transform(X)\n",
        "\n",
        "# window size, etc.\n",
        "window_sizes = [16, 32, 64, 128]\n",
        "compressions = [4, 8, 16]\n",
        "methods = ['DFT', 'DCT', 'DWT', 'PAA','MINE']\n",
        "\n",
        "#file for storing\n",
        "folder_name = f\"Linear Regression PUMP Measurements\"\n",
        "if not os.path.exists(folder_name):\n",
        "   os.makedirs(folder_name)\n",
        "\n",
        "raw_folder_name = f\"RAW DATA\"\n",
        "if not os.path.exists(os.path.join(folder_name,raw_folder_name)):\n",
        "   os.makedirs(os.path.join(folder_name,raw_folder_name))\n",
        "pdf_file = os.path.join(folder_name,raw_folder_name, 'plots.pdf')\n",
        "\n",
        "with PdfPages(pdf_file, 'a') as pdf:\n",
        "  for window_size in window_sizes:\n",
        "\n",
        "     # lists to store metrics for each window\n",
        "     rmse_list = []\n",
        "     #mae_list = []\n",
        "     r2_list = []\n",
        "     num =int( window_size+window_size/4)\n",
        "\n",
        "     # Iterate over the windows\n",
        "     # Iterate over the windows\n",
        "     for i in range(0, len(X) - window_size * 2 + 1, window_size):\n",
        "        # Split the data into training and testing sets\n",
        "       X_train = X[i:i + window_size]\n",
        "       X_test = X[i + window_size:i + num]\n",
        "       y_train = y[i:i + window_size]\n",
        "       y_test = y[i + window_size:i + num]\n",
        "\n",
        "        # Data scaling\n",
        "       sc = MinMaxScaler()\n",
        "       X_train_scaled = sc.fit_transform(X_train)\n",
        "       X_test_scaled = sc.transform(X_test)\n",
        "\n",
        "       # Model application\n",
        "       reg_line = LinearRegression()\n",
        "       reg_line.fit(X_train_scaled, y_train)\n",
        "        # Generate predictions\n",
        "       y_pred_scaled = reg_line.predict(X_test_scaled)\n",
        "\n",
        "       # performance (mean squared error and R-squared score)\n",
        "       mse = mean_squared_error(y_test, y_pred_scaled)\n",
        "       rmse = np.sqrt(mse)\n",
        "       r2 = r2_score(y_test, y_pred_scaled)\n",
        "\n",
        "       rmse_list.append(rmse)\n",
        "       r2_list.append(r2)\n",
        "\n",
        "     #store plots\n",
        "     methd = 'RAW'\n",
        "     comprsn = 0\n",
        "     fig = plot_confidence_interval(r2_list,methd,window_size ,comprsn,'SC') #plot function call\n",
        "     pdf.savefig(fig)\n",
        "     plt.close(fig)\n",
        "\n",
        "     fig2 = plot_confidence_interval(rmse_list,methd,window_size ,comprsn,'RMSE') #plot function call\n",
        "     pdf.savefig(fig2)\n",
        "     plt.close(fig2)\n",
        "\n",
        "     # create a file name based on the method, subset index, stride, and compression\n",
        "     avg_r2 = np.mean(r2_list)\n",
        "     file_name = f\"Window size({window_size}) - R2-scores.txt\"\n",
        "     with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Average R2 Score: {avg_r2}\\n\")\n",
        "              content = str(r2_list)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "     avg_rmse = np.mean(rmse_list)\n",
        "     file_name = f\"Window size({window_size}) - RMSE scores).txt\"\n",
        "     with open(os.path.join(folder_name,raw_folder_name, file_name), 'w+') as f:\n",
        "              f.write(f\"Average RMSE Score: {avg_rmse}\\n\")\n",
        "              content = str(rmse_list)\n",
        "              f.write(content)\n",
        "              f.close()\n",
        "\n",
        "for window_size in window_sizes:\n",
        "\n",
        "    #file for storing\n",
        "    stride_folder_name = f\"stride({window_size})\"\n",
        "    if not os.path.exists(os.path.join(folder_name,stride_folder_name)):\n",
        "        os.makedirs(os.path.join(folder_name,stride_folder_name))\n",
        "\n",
        "    for compression in compressions:\n",
        "\n",
        "        #vector and inverse vector arrays for our algorithm\n",
        "        n = 64*int(window_size/compression)  # Dimension of each random vector\n",
        "        r = 51  # Number of random vectors to generate/rows\n",
        "        R = generate_random_vectors(n, r)\n",
        "        R_inv= np.linalg.pinv(R)\n",
        "\n",
        "        #files for storing measurements\n",
        "        compression_folder_name = f\"compression({compression})\"\n",
        "        if not os.path.exists(os.path.join(folder_name,stride_folder_name, compression_folder_name)):\n",
        "            os.makedirs(os.path.join(folder_name,stride_folder_name, compression_folder_name))\n",
        "\n",
        "        #files for storing plots\n",
        "        plots_dir = os.path.join(folder_name,stride_folder_name, compression_folder_name, 'plots')\n",
        "        os.makedirs(plots_dir, exist_ok=True)\n",
        "        pdf_file = os.path.join(plots_dir, 'plots.pdf')\n",
        "\n",
        "        with PdfPages(pdf_file, 'a') as pdf:\n",
        "            results1 = []\n",
        "            results2 = []\n",
        "            for method in methods:\n",
        "\n",
        "                slices = window_size // compression\n",
        "\n",
        "                # lists to store metrics for each window\n",
        "                rmse_list = []\n",
        "                #mae_list = []\n",
        "                r2_list = []\n",
        "                num =int( window_size+window_size/4)\n",
        "\n",
        "                # Iterate over the windows\n",
        "                for i in range(0, len(X) - window_size * 2 + 1, window_size):\n",
        "                   # Split the data into training and testing sets\n",
        "                   X_train = X[i:i + window_size]\n",
        "                   X_test = X[i + window_size:i + num]\n",
        "                   y_train = y[i:i + window_size]\n",
        "                   y_test = y[i + window_size:i + num]\n",
        "\n",
        "                   # Data scaling\n",
        "                   sc = MinMaxScaler()\n",
        "                   X_train_scaled = sc.fit_transform(X_train)\n",
        "                   X_test_scaled = sc.transform(X_test)\n",
        "\n",
        "                   #print(X_train_scaled[0], X_test_scaled[0], y_train_scaled[0], y_test_scaled[0])\n",
        "\n",
        "                   if method == 'DFT':\n",
        "\n",
        "                       #array to store the restored data\n",
        "                       compressed_X_train_scaled = np.zeros_like(X_train_scaled)\n",
        "\n",
        "                       for i in range(window_size):\n",
        "                          # Compute abs DFT of each row\n",
        "                          dft_X_train_scaled  =  np.fft.fft(X_train_scaled[i])\n",
        "                          # Sort the DFT coefficients along each row by their magnitude\n",
        "                          sorted_indices = np.argsort(-np.abs(dft_X_train_scaled))[:slices]\n",
        "                          sorted_dft_X_train_scaled = dft_X_train_scaled[sorted_indices]\n",
        "                          # Keep  top  coeff\n",
        "                          top__dft_X_train_scaled = sorted_dft_X_train_scaled[:slices]\n",
        "                          #reconstruct the compressed dataset\n",
        "                          compressed_X_train_scaled[i] = np.fft.ifft(top__dft_X_train_scaled,51).real\n",
        "\n",
        "                       #array to store the restored data\n",
        "                       compressed_X_test_scaled = np.zeros_like(X_test_scaled)\n",
        "\n",
        "                       for i in range(int(window_size/4)):\n",
        "                          # Compute abs DFT of each row\n",
        "                          dft_X_test_scaled =  np.fft.fft(X_test_scaled[i])\n",
        "                          # Sort the DFT coefficients along each row by their magnitude\n",
        "                          sorted_indices = np.argsort(np.abs(dft_X_test_scaled))[::-1][:slices]\n",
        "                          sorted_dft_X_test_scaled = dft_X_test_scaled[sorted_indices]\n",
        "                          # Keep  top  coeff\n",
        "                          top__dft_X_test_scaled = sorted_dft_X_test_scaled[:slices]\n",
        "                          #reconstruct the compressed dataset\n",
        "                          compressed_X_test_scaled[i] = np.fft.ifft(top__dft_X_test_scaled,51).real\n",
        "\n",
        "                   elif method == 'DCT':\n",
        "\n",
        "                       #array to store the restored data\n",
        "                       compressed_X_train_scaled = np.zeros_like(X_train_scaled)\n",
        "\n",
        "                       for i in range(window_size):\n",
        "                          # Compute abs DCT of each row\n",
        "                          dct_X_train_scaled  =  dct(X_train_scaled[i])\n",
        "                          # Sort the DCT coefficients along each row by their magnitude\n",
        "                          sorted_indices = np.argsort(-np.abs(dct_X_train_scaled))[:slices]\n",
        "                          sorted_dct_X_train_scaled = dct_X_train_scaled[sorted_indices]\n",
        "                          # Keep  top  coeff\n",
        "                          top__dct_X_train_scaled = sorted_dct_X_train_scaled[:slices]\n",
        "                          #reconstruct the compressed dataset\n",
        "                          compressed_X_train_scaled[i] = idct(top__dct_X_train_scaled ,type = 2, n = 51).real\n",
        "\n",
        "                       #array to store the restored data\n",
        "                       compressed_X_test_scaled = np.zeros_like(X_test_scaled)\n",
        "\n",
        "                       for i in range(int(window_size/4)):\n",
        "                          # Compute abs DCT of each row\n",
        "                          dct_X_test_scaled =  dct(X_test_scaled[i])\n",
        "                          # Sort the DCT coefficients along each row by their magnitude\n",
        "                          sorted_indices = np.argsort(-np.abs(dct_X_test_scaled))[:slices]\n",
        "                          sorted_dct_X_test_scaled = dct_X_test_scaled[sorted_indices]\n",
        "                          # Keep  top  coeff\n",
        "                          top__dct_X_test_scaled = sorted_dct_X_test_scaled[:slices]\n",
        "                          #reconstruct the compressed dataset\n",
        "                          compressed_X_test_scaled[i] = idct(top__dct_X_test_scaled ,type = 2 , n = 51).real\n",
        "\n",
        "\n",
        "                   elif method == 'DWT':\n",
        "\n",
        "                      #array to store the restored data\n",
        "                      compressed_X_train_scaled = np.zeros_like(X_train_scaled)\n",
        "                      compressed_X_train_scaled = np.pad(compressed_X_train_scaled, ((0, 0), (0, 1)), 'constant', constant_values=0)\n",
        "                      for i in range(window_size):\n",
        "                          # Apply DWT to each row using 'db1' wavelet\n",
        "                          cA, cD = pywt.dwt(X_train_scaled[i], 'db1')\n",
        "                          # Sort\n",
        "                          sorted_cD_subset_data = np.zeros_like(cD) #initialize storage array\n",
        "                          sorted_indices = np.argsort(-np.abs(cD))[:slices]\n",
        "                          sorted_cD_subset_data[sorted_indices] = cD[sorted_indices]\n",
        "                          # Perform inverse DWT to restore the row\n",
        "                          compressed_X_train_scaled[i] = pywt.idwt(cA, sorted_cD_subset_data, 'db1')\n",
        "\n",
        "                      #array to store the restored data\n",
        "                      compressed_X_test_scaled = np.zeros_like(X_test_scaled)\n",
        "                      compressed_X_test_scaled = np.pad(compressed_X_test_scaled, ((0, 0), (0, 1)), 'constant', constant_values=0)\n",
        "                      for i in range(int(window_size/4)):\n",
        "                          # Apply DWT to each row using 'db1' wavelet\n",
        "                          cA, cD = pywt.dwt(X_test_scaled[i], 'db1')\n",
        "                          # Sort\n",
        "                          sorted_cD_subset_data = np.zeros_like(cD) #initialize storage array\n",
        "                          sorted_indices = np.argsort(-np.abs(cD))[:slices]\n",
        "                          sorted_cD_subset_data[sorted_indices] = cD[sorted_indices]\n",
        "                          # Perform inverse DWT to restore the row\n",
        "                          compressed_X_test_scaled[i] = pywt.idwt(cA, sorted_cD_subset_data, 'db1')\n",
        "\n",
        "\n",
        "                   elif method == 'PAA':\n",
        "                      # Apply PAA along the rows of the array\n",
        "                      paa = PAA(window_size = compression)\n",
        "                      compressed_X_train_scaled = paa.fit_transform(X_train_scaled)\n",
        "                      compressed_X_test_scaled = paa.fit_transform(X_test_scaled)\n",
        "\n",
        "                   elif method == 'MINE':\n",
        "\n",
        "                      #array to store the restored data\n",
        "                      compressed_X_train_scaled = np.zeros_like(X_train_scaled)\n",
        "\n",
        "                      for i in range(window_size):\n",
        "                        row = X_train_scaled[i]\n",
        "                        # Apply my algorithm along the rows of the array\n",
        "                        compressed_X_train_scaled[i] = MINE(row,R,R_inv)\n",
        "\n",
        "                      #array to store the restored data\n",
        "                      compressed_X_test_scaled = np.zeros_like(X_test_scaled)\n",
        "                      for i in range(int(window_size/4)):\n",
        "                        row = X_test_scaled[i]\n",
        "                        # Apply my algorithm along the rows of the array\n",
        "                        compressed_X_test_scaled[i] = MINE(row,R,R_inv)\n",
        "\n",
        "                   # Model application\n",
        "                   regressor = LinearRegression()\n",
        "                   regressor.fit(compressed_X_train_scaled, y_train)\n",
        "\n",
        "                   # Generate predictions\n",
        "                   y_pred_scaled = regressor.predict(compressed_X_test_scaled)\n",
        "\n",
        "                   # Transform scaled predictions back to og scale\n",
        "                    #y_pred = sc.inverse_transform(y_pred_scaled)\n",
        "\n",
        "                   # performance (mean squared error, mean absolute error, and R-squared score)\n",
        "                   mse = mean_squared_error(y_test, y_pred_scaled)\n",
        "                   rmse = np.sqrt(mse)\n",
        "                    #mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
        "                   r2 = r2_score(y_test, y_pred_scaled)\n",
        "\n",
        "                   # Add to list of measurements\n",
        "                   rmse_list.append(rmse)\n",
        "                   #mae_list.append(mae)\n",
        "                   r2_list.append(r2)\n",
        "\n",
        "                results1.append((r2_list, method))\n",
        "                results2.append((rmse_list, method))\n",
        "\n",
        "                # create a file name based on the method, subset index, stride, and compression\n",
        "                avg_r2 = np.mean(r2_list)\n",
        "                file_name = f\"{method}_window size({window_size})_compression({compression} - R2 SCORES).txt\"\n",
        "                # write the silhouette coefficients to file\n",
        "                with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "                              f.write(f\"Average R2-Score: {avg_r2}\\n\")\n",
        "                              content = str(r2_list)\n",
        "                              f.write(content)\n",
        "                              f.close()\n",
        "\n",
        "                avg_RMSE = np.mean(rmse_list)\n",
        "                file_name = f\"{method}_window size({window_size})_compression({compression} - RMSEs).txt\"\n",
        "                # write the silhouette coefficients to file\n",
        "                with open(os.path.join(folder_name,stride_folder_name, compression_folder_name, file_name), 'w+') as f:\n",
        "                              f.write(f\"Average RMS-Score: {avg_RMSE}\\n\")\n",
        "                              content = str(rmse_list)\n",
        "                              f.write(content)\n",
        "                              f.close()\n",
        "            #store plots\n",
        "            fig = plot_score_confidence_interval(results1,window_size,compression) #plot function call\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "            fig = plot_RMSE_confidence_interval(results2,window_size,compression) #plot function call\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J9DuIq60Eeqn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "import sys\n",
        "from scipy import stats\n",
        "from scipy.fft import fft, dct\n",
        "import pywt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "def paa(data, n_pieces):\n",
        "    \"\"\"\n",
        "    Piecewise Aggregate Approximation (PAA) on data.\n",
        "    \"\"\"\n",
        "    n = data.shape[0]\n",
        "    piece_length = int(np.ceil(n/n_pieces))\n",
        "    padded_data = np.pad(data, ((0, piece_length*n_pieces-n), (0,0)), mode='constant', constant_values=0)\n",
        "    pieces = padded_data.reshape(n_pieces, piece_length, -1)\n",
        "    return np.mean(pieces, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/pump_sensor2.txt', delimiter=',')\n",
        "\n",
        "# BROKEN,NORMAL,RECOVERING -> 0,0.5,1\n",
        "le = LabelEncoder()\n",
        "df.iloc[:, -1] = le.fit_transform(df.iloc[:, -1])\n",
        "max_label = df.iloc[:, -1].max()\n",
        "df.iloc[:, -1] = df.iloc[:, -1] / max_label * 1.0\n",
        "# Discard the first row + first two columns\n",
        "df = df.iloc[:, 2:]\n",
        "data = df.to_numpy(dtype='float')\n",
        "# Split  features (X) and target (y)\n",
        "X = data[:, :-1]\n",
        "#print(X)\n",
        "y = data[:, -1]\n",
        "#print(y[0])\n",
        "# Fill Nan values based on the mean of each sensor\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "# Select 1000 0.5 kai 1 labels\n",
        "label_0_indices = np.where(y == 0)[0]\n",
        "label_05_indices = np.where(y == 0.5)[0]\n",
        "label_1_indices = np.where(y == 1)[0]\n",
        "label_05_indices = np.random.choice(label_05_indices, size=1000, replace=False)\n",
        "label_1_indices = np.random.choice(label_1_indices, size=1000, replace=False)\n",
        "# Concatenate + shuffle\n",
        "indices = np.concatenate((label_0_indices, label_05_indices, label_1_indices))\n",
        "np.random.seed(123)   #Gia idio dataset shuffle se ola ta algorithms\n",
        "np.random.shuffle(indices)\n",
        "X= X[indices]\n",
        "y= y[indices]\n",
        "#print\n",
        "# unique_labels, label_counts = np.unique(y, return_counts=True)\n",
        "# for i in range(len(unique_labels)):\n",
        "#    print(f\"Label {unique_labels[i]}: {label_counts[i]}\")\n",
        "# X = imputer.fit_transform(X)\n",
        "\n",
        "# window size, etc.\n",
        "window_size = 128 #@param {type:\"number\"}\n",
        "method = 4              #@param {type:\"number\"} 1=DFT , 2=DCT, 3=discrete wavelet transform , 4=PAA\n",
        "num = 0\n",
        "num =int( window_size+window_size/4)\n",
        "\n",
        "# lists to store metrics for each window\n",
        "mse_list = []\n",
        "mae_list = []\n",
        "r2_list = []\n",
        "subset_number = 1\n",
        "\n",
        "# Iterate over the windows\n",
        "for i in range(0, len(X)-window_size*2+1, window_size):\n",
        "\n",
        " # Split the data into training and testing sets\n",
        " X_train = X[i:i+window_size]\n",
        " X_test = X[i+window_size:i+num]\n",
        " y_train = y[i:i+window_size]\n",
        " y_test = y[i+window_size:i+num]\n",
        " print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        " # Data scaling\n",
        " sc = MinMaxScaler()\n",
        " X_train_scaled = sc.fit_transform(X_train)\n",
        " X_test_scaled = sc.transform(X_test)\n",
        " y_train_scaled = sc.fit_transform (y_train.reshape(y_train.shape[0],1))\n",
        " y_test_scaled = sc.transform (y_test.reshape(y_test.shape[0],1))\n",
        " #print(X_train_scaled[0], X_test_scaled[0], y_train_scaled[0], y_test_scaled[0])\n",
        "\n",
        " if method == 1:\n",
        "        X_train_scaled = np.abs(fft(X_train_scaled, axis=0))\n",
        "        X_test_scaled = np.abs(fft(X_test_scaled, axis=0))\n",
        "        y_train_scaled = np.abs(fft(y_train_scaled, axis=0))\n",
        "        y_test_scaled = np.abs(fft(y_test_scaled, axis=0))\n",
        "        print_string = 'DFT'\n",
        " elif method == 2:\n",
        "        X_train_scaled = dct(X_train_scaled, axis=0)\n",
        "        X_test_scaled = dct(X_test_scaled, axis=0)\n",
        "        y_train_scaled = dct(y_train_scaled, axis=0)\n",
        "        y_test_scaled = dct( y_test_scaled, axis=0)\n",
        "        print_string = 'DCT'\n",
        " elif method == 3:\n",
        "        cA, cD = pywt.dwt(X_train_scaled, 'db1', axis=0)\n",
        "        X_train_scaled = np.concatenate((cA, cD), axis=0)\n",
        "        cA, cD = pywt.dwt(X_test_scaled, 'db1', axis=0)\n",
        "        X_test_scaled = np.concatenate((cA, cD), axis=0)\n",
        "        cA, cD = pywt.dwt(y_train_scaled, 'db1', axis=0)\n",
        "        y_train_scaled = np.concatenate((cA, cD), axis=0)\n",
        "        cA, cD = pywt.dwt(y_test_scaled, 'db1', axis=0)\n",
        "        y_test_scaled = np.concatenate((cA, cD), axis=0)\n",
        "        print_string = 'DWT'\n",
        " elif method == 4:\n",
        "        X_train_scaled = paa(X_train_scaled, n_pieces=8)\n",
        "        X_test_scaled = paa(X_test_scaled, n_pieces=8)\n",
        "        y_train_scaled = paa(y_train_scaled, n_pieces=8)\n",
        "        y_test_scaled = paa( y_test_scaled, n_pieces=8)\n",
        "        print_string = 'PAA'\n",
        " else   :\n",
        "      print_string = 'RAW'\n",
        "\n",
        " # Model application\n",
        " reg_line = LinearRegression()\n",
        " reg_line.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        " # Generate predictions\n",
        " y_pred_scaled = reg_line.predict(X_test_scaled)\n",
        "\n",
        " # Transform scaled predictions back to og scale\n",
        " #y_pred = sc.inverse_transform(y_pred_scaled)\n",
        "\n",
        " # performance (mean squared error, mean absolute error, and R-squared score)\n",
        " mse = mean_squared_error(y_test_scaled, y_pred_scaled)\n",
        " mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
        " r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
        "\n",
        " #print(f\"Subset {subset_number} : | MSE= {mse:.5f}, | MAE= {mae:.5f}, R-squared= {r2:.5f}\")\n",
        " subset_number= subset_number + 1\n",
        " # Add to list of measurements\n",
        " mse_list.append(mse)\n",
        " mae_list.append(mae)\n",
        " r2_list.append(r2)\n",
        "\n",
        "# Saving the scores array in a text file\n",
        "file_name = f\"Linear Regression({window_size}) {print_string} measurements.txt\"\n",
        "file = open(file_name, \"w+\")\n",
        "content = str(r2_list)\n",
        "file.write(content)\n",
        "file.close()\n",
        "\n",
        "#plot\n",
        "\n",
        "# mean silhouette score and confidence interval\n",
        "mean_score = np.mean(r2_list)\n",
        "ci = stats.t.interval(0.95, len(r2_list)-1, loc=mean_score, scale=stats.sem(r2_list))\n",
        "\n",
        "# 2D array with the lower and upper bounds of the confidence interval\n",
        "yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "# Plot the data with error bars\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(r2_list, marker='o')\n",
        "ax.errorbar(x=range(len(r2_list)), y=r2_list, yerr=yerr, fmt='none', ecolor='r')\n",
        "ax.set_xlabel('Subset of features')\n",
        "ax.set_ylabel('R2 score')\n",
        "ax.set_title('R2 scores with 95% confidence interval')\n",
        "plt.show()\n",
        "\n",
        "print(f\"95% confidence interval: [{ci[0]:.5f}, {ci[1]:.5f}]\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6Mf3Flb3pOV"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        " #plot\n",
        "\n",
        "# mean silhouette score and confidence interval\n",
        "mean_score = np.mean(r2_list)\n",
        "ci = stats.t.interval(0.95, len(r2_list)-1, loc=mean_score, scale=stats.sem(r2_list))\n",
        "\n",
        "# 2D array with the lower and upper bounds of the confidence interval\n",
        "yerr = np.array([[mean_score-ci[0]], [ci[1]-mean_score]])\n",
        "\n",
        "# Plot the data with error bars\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(r2_list, marker='o')\n",
        "ax.errorbar(x=range(len(r2_list)), y=r2_list, yerr=yerr, fmt='none', ecolor='r')\n",
        "ax.set_xlabel('Subset of features')\n",
        "ax.set_ylabel('R2 score')\n",
        "ax.set_title('R2 scores with 95% confidence interval')\n",
        "plt.show()\n",
        "\n",
        "print(f\"95% confidence interval: [{ci[0]:.5f}, {ci[1]:.5f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MOFacAW5g1aJ"
      },
      "outputs": [],
      "source": [
        "#@title Whole file\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "import sys\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/pump_sensor2.txt', delimiter=',')\n",
        "\n",
        "# BROKEN,NORMAL,RECOVERING -> 0,1,2\n",
        "le = LabelEncoder()\n",
        "df.iloc[:, -1] = le.fit_transform(df.iloc[:, -1])\n",
        "\n",
        "# Discard the first row + first two columns\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "data = df.to_numpy(dtype='float')\n",
        "\n",
        "# Split  features (X) and target (y)\n",
        "X = data[:, :-1]\n",
        "#print(X)\n",
        "y = data[:, -1]\n",
        "#print(y[0])\n",
        "\n",
        "# Fill Nan values based on the mean of each sensor\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=13)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "# Data scaling\n",
        "sc = MinMaxScaler()\n",
        "X_train_scaled = sc.fit_transform(X_train)\n",
        "X_test_scaled = sc.transform(X_test)\n",
        "\n",
        "y_train_scaled = sc.fit_transform (y_train.reshape(y_train.shape[0],1))\n",
        "y_test_scaled = sc.transform (y_test.reshape(y_test.shape[0],1))\n",
        "#print(X_train_scaled[0], X_test_scaled[0], y_train_scaled[0], y_test_scaled[0])\n",
        "\n",
        "# Model application\n",
        "reg_line = LinearRegression()\n",
        "reg_line.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Generate predictions\n",
        "y_pred_scaled = reg_line.predict(X_test_scaled)\n",
        "\n",
        "# Transform scaled predictions back to og scale\n",
        "y_pred = sc.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Performance (mean squared error, mean absolute error, and R-squared score)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE: \", mse)\n",
        "print(\"MAE: \", mae)\n",
        "print(\"R-squared: \", r2)\n",
        "\n",
        "# Plot the predicted values against the actual values\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"Actual values\")\n",
        "plt.ylabel(\"Predicted values\")\n",
        "plt.title(\"Linear Regression Results\")\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}